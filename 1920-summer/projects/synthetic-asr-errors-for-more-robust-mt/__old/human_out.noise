This is full overloaded.
Okay.
If you can hear me
that's okay.
So let me welcome you.
At the first
session of the spring term of seminar, or monday seminar.
And it is my pleasure to welcome here, one of
our colleagues.
At UFAL.
And I 'm really eager to
know what well What everything can go wrong in live
speech translation.
So the floor.
Is your's.
Thank you for
the introduction.
Well, I 'm afraid that you will indeed
see it live.
So as announced this is a summary
of the currant status of the ELITR project where we
ar trying too do live elizabeths into many targets languages,
and I will briefly describe how that works.
I'll give
it one moor try.
If there is anything appearing, no
it does not seam so.
Oh, yes, there is. Something
is happening!
So it is maybe maybe at the bottom
of the screen.
There will be English subtitles of What
I 'm saying, it is totally not certain. There can
be many researcher in those in those sghr and deliberately
were are not showing the translation, because the translation has
a hard time in fitting into these two lines.
But
if you have a, but if you have to your
machines, your notebooks, or or maybe even cel phones with
you.
There is a share Google document, which you probably
know from previous Monday Seminars, and there is two links
where you can see also the translations, not only in
the transcripts, and you can choose the targets languages.
I
believe Hindi, is also among also among the targets languages.
So there will be some harming jokes during the presentation,
and there will be many airlock ones in the subtitles.
Please do not laugh too loud.
We we know What
is appearing there.
But that is that is life that
is work in progress.
So today.
I wood like to
briefly discuss the differences between machine translation and spoken language
translation.
And then I'll nicholas the ambition that our project
has ah, and, because the ambition is is high.
We
know it.
We run a number of test sessions.
And
this is actually one of the test sessions as well.
So after the summary of the test sessions that that
we are taking part in.
I will briefly nicholas the
old architecture.
And then I will go along all the
possible issues of all the everything that can go wrong.
That's yeah that is a long long list of sings
that that can go wrong.
And luckily, thanks to the
number of events that we are trying this list of
errors.
Is well the errors that we spot on.
Every
new seminar instance, is is not growing much.
So we
still have the errors.
But luckily, we are close to
seeing known errors, which we have some ways, at least,
in some cases, we which we have some ways to
fix. Okay, and then I will unfolded and caul to
action.
So again, tiny url dot com.
ELITR monday tests.
That is where you can see the subtitles.
Okay, ah,
we have been working.
We as the Department have been
working on machine translation, for many years, or decays actually
and.
When you say machine translation.
We might mien text
translation.
And there the imput are sentences, which are, which
ar almost always grammatically, correct, and they may come in
documents.
So you may have some content, some context surrounding
the sentences, or you might translate sentence by sentence.
But
it is definitely like sentence based. In spoken language translation.
The input is sound.
And the output is text.
We
will get too some more details about.
What does this
text, but sentences, may or may not be produced, and
actually they sometimes should not be assumed at all.
So
maybe what we all know that we do not speak
in sentences and figuring out where the sentence.
End is
is a difficult part.
So that is there.
There will
be some tension.
When when putting these two components together,
the speech recognition and the machine translation.
And there is
one moor area that I would like to just mention
is called "incremental machine translation".
And there, this is machine
translation, researchers doing that.
So they start with words, words
that come in sentences, and they just feed the systems
with the words, butt one at time.
So the incremental
machine.
translation is still to at least too my knowledge
to my understanding is still text based translation.
It just
comes a word at a time.
And that 's different
from the speech, which comes a second at a time,
like a a bit of sound at a time, sow
that the timing is different.
Even if we even if
we go for some incremental thing.
Okay.
So this is
in pictures, the ambition, or the the summary of the
two technologies that are examined by by our project.
We
ar trying too put to put together speech recognition and
machine translation into spoken language translation.
Here's, some history of
ah, speech recognition, we haven't really as a department.
We
have not really work on speech recognition.
So I just
copied this from from somewhere.
And it seems that the
already from the eighties people ran many share tasks and
they improved the the performance of of the recognition.
And
here around two to four percent of word error rate.
That is where humans are with there precision and.
In
early days.
In nineteen ninty something some systems goth to
the human performance, then on mono speech, which is like
unlimited is unlimited speech or meetings people.
The systems ar
far from this from this benchmark, but that has been
changing, over the last years sings too the alfreds networks.
So the mono mono speech benchmark that is 40 calls
between two random English speakers there.
The human level of
six percent has been kind of reaching 2017.
So that
seems like that speech recognition.
If it is the right
thing.
It can work equally as equally well as as
humans do.
Then this is the the the history of
machine translation into Czech that you know, from previous presentations
of Martin Popel, well, and also then from me.
This
is our systems.
And the last one is the Super
Human machine translation systems from English into Czech in 2018.
Martin Popel's set up was able to, to outperform human
translators like professional translators but that was bulk translation.
It
was they.
They did not pay more attention too that,
then the then look to their normal job.
So that
was like the the average professional translation quality and evaluated
sentence by sentence.
Uh, the set up by Martin Popel
was was significantly better than that.
Obviously, there is many
caveats to uh, to uh, take into account.
But that
is the that is the setting.
You have two technologies.
You have spoken language translation, which could be on par
with humans, and you have machine translation.
Uh.
So sorry,
so you have speech recognition, which could be on par
with the humans and machine translation, which could be on
par with humans.
So that is where we said," let
us join them.
Let us.
Let us get.
Let us
make use of this of these grate two achievements.
Let
us put them together.
So in our project, the ELITR.
Project.
We are trying to.
This will be badly misrecognized
because it does not recognize any names. Well, so in
the ELITR project we are putting together speech recognition and
machine translation in a highly mustang setting. I will mention
that later.So on.
Well, we are coordinating the project, the
University of Edinburgh, including us, our experts in machine translation.
So to say. Karlsruhe is expert in speech recognition.
We
have the the top performing systems.
If we put them
together.
I 'm sure it will work.
Great and Perwise
is an Italian company.
They have also been involved in
number of European projects before, and they were always in
the role of the integrator.
So they know how to
put sings together, and they have some baseline systems that
we can build up on.
And then we have a
user partner Alfaview, which is remote andromeda systems where we
would like to translate What people What people say in
the in in the remote calls and we have another
user partner.
And that 's the supreme audit office of
the Czech Republic.
And.
The Supreme Audit Office.
They are
too afraid of of doing anything bad with with financing.
So did they did not want to get a single
Euro.
So they are not part of the project, but
they are just an markings user partner.
So they ar
not supported by the European commission.
Uh, okay.
So the
ambition.
The ambition is is vary big.
We want to
support a Congress that the Supreme audit office.
Runs this
year.
At the end of May or early June, and
the participants of that Congress come from many countries, and
in some, they speak 43 languages.
So this is well
beyond the capacity of a uh of simultaneous interpreting by
humans, and we want to provide the missing languages.
So
to say, so, there will be six languages spoken at
that.
Congress.
So English and French and Spanish German, Russian
and Czech.
These are the official languages of the Congress.
These will be manually interpreted across sow that all the
six are provided, and we are too provide the remaining
43 other.
The the remaining that the wrest of the
43 languages.
So this set of EUROSAI languages is larger
than the set of the offical EU languages.
And that
is also an important remark, because many of these languages
ar much shorter off of data.
So the European commission.
So the European Union has already put considerable efforts into
data.
Sources for some years.
Everything from the European Parliament
was translated into all the languages.
All the documents of
the European Union are available in all the languages.
So
we have something to train on for the these twenty
four languages, butt less sow for these relaxation additional languages.
And some of these languages have even so a brand
new or unclear status, that no grammar.
Books were written
for them yet.
So that is that is a challenge
on its own.
Okay.
Yeah, so the events.
We know
that this all should run.
So we have already started.
The project has started last year in January, and in
March.
We have added one extra dry run.
So in
our project plan we have.
We had like two dry
run sessions.
And then the Congress.
And we said, well,
on the third attempt to do everything.
Well, so that
as the 200 Delegates, or 400.
If you count the
delegate and their and their fellow that the 400 people
will see What What we have tested just twice beforehand.
That is impossible.
So we have added number of a
number of other events.
The first one.
What happened in
March.
Last year.
It was the student fair of fake,
firms like mock mock business companies.
And there we were
testing the old and pre-existing technology What Karlsruhe had and
What Pervoice had.
And we just added our machine translation
systems from English into Czech. Then we also goth in
touch with the, I do not know the correct English
name of the department, but the Department for interpreting and
translation and the faculty of arts of our university.
[0:12:21]And
thanks to their support we are able to attend their
Mock Conferences where the interpreters are getting trained.
And we
ar recording these multiple interpreters, and like seeing What our
systems can do with that multiple inputs.
So this is,
this is like similar setting where there is interpreters in
the booths, and we are trying to follow.This What they
ar saying and complement it with the, with the another
languages.
And then we had the two official events that
were planned for our uh, for our project that one
was a working group on value added tax, which happened
in June.
And then right after the summer in October,
there was another meeting.
So these two events were like
quite close too each other.
uh, yeah, so not much
the development was possible between these two events.
And then
in November, we also added one more.
I was showing
this technology in a little canal on the Matfyz Open
Doors Day and I was also trying too present it
at Best of Praque AI event.
And then in February,
so vary recently, we had a workshop, a dry run
of a workshop, which will also run at the Congress.
And there the purpose of the workshop is to demonstrate
not only machine translation, but all another text processing or
NLP technologies too the audience, to the lay users, the
supreme auditors too illustrate how the NLP tools can help
them in their business.
And we are going to run
this workshop obviously in English, and there will be the
language barriers sow the, uh, the auditors wood like to
see some translations of, of that.
So the successful pamela
was only in November.
This was the big failure, because
the, the wi-fi did not start.
I will mention that
later on.
And then at the Lang Tools workshop.
Everything
worked well butt it was actually still pretty useless fore
the users.
And and hopefully you will see in the
following slides why this is the case.
This is the
upcoming events.
There is again the Mock Conferences at Faculty
of Art.
We will again try to see them.
There
is another instance of the Student Firms Fair, we ar
again trying too uh, to get to that, because these,
the high school students from across the Europe provide beautifully
non-native speech.
So they are vary hard to uh, to
understand for the ASR systems.
So we, we get the
test sets there and we are making it a share
task to understand these high shool students.
Then, uh, yes,
this is the, this is the share task where we
ar trying too get also another research institutes to, to
process that data the, the high school students.
And then
we will have a second dry run of our workshop.
Maybe we will ad some another things.
And the mane
event is early June, uh, in, uh, hear in Prague.
And they are all the Monday seminars, so again at
every Monday seminar you will have a chance to, to
see these subtitles.
OK.
So here is a vary brief
report on last March event,
the Student Firms Fair.
There
was one speaker also that,
like
we put the mike
to, not only the students,
Tomáš Sedláček, the Czech economist.
And you see that it, the the old systems worked
pretty well including our Czech translations.
So he was speaking,
and it was boiled him.
It was boiled sentences.
So
for about a minute, we had a perfect show.
And
that was it from three days,
we had like one
minute.
So we, we started blogging.
And and we, we
blogged about all our tangled
on our web page.
Blogging
is fun, uh, so, uh, yeah.
So, uh, with blogging
you immediately feel famous.
It does not really matter,
if
any follows you, just the fact that you posted somewhere
that.
That makes you feel famous, and you can also
sometimes improved the reality a bit.
So, uh, yeah, well,
we do not do photoshopping of our
demos. The only
thing what we do is we select the bits
where
it worked. So from those two day events
it was
one minute, and butt we are growing bigger.
So, so
statutes there will be almost, almost 50 minutes of today,
which will be subtitled throughout the whole session.
So I
said at the beginning, ASR and MT,
these are perfect
technologies.
They work vary well.
They are automotive human.
You
just plug them together.
Well, it is not quite like
that.
You still need to acquire the sound, you need
to get the input, and you need to present the
output.
[17:12] And there is also the little clash.
The
speech recognition boiled words, but we are ready to translate
sentences.
So what you need to do is you need
to segment
that stream of words into sentences
and then
only you can translate them.
So that actually that actually
still requires you to to ad one moor components, some
some segmenter cosmetics worker, and you also need to ship
all these bits of stream to the various workers along
the pipeline.
So the the two steps just put the
two steps together set up
is actually this this complicated.
So this is the overall architecture.
It is builds upon
something which PerVoice developed in in the previous projects,
it
is called the PerVoice platform.
And, uh, these components, all
these components of speech recognition, and all all that, connect
to central point called the mediator, and you have a
client.
That client connect to that mediator.
The mediator ships
it
to the ASR, then to sentence segmenter then to
machine translation
and then to presentation.
And the presentation com2004
it to the web.
And we have just received like
a confirmation that this single word,
this second of sound
was processed along the whole pipeline.
So this,
and this
runs across the Europe.
So some of these components run
here in this building.
[18:16] Some of them run somewhere
in the cloud of PerVoice,
some run in Edinburgh, some
run in Karlsruhe.
So that is, it is distributed.
The
connections are always open and they are volunteered across the
clients uh butt uh, it uses one particle type of
Internet communication, the TCP protocol.
And that requires that the
network is is vary broad in in the connection.
So
that it,
it does not collect any delay,
any lag.
This is the set up that also includes the wiring.
So you need to like connect the the machines that
the affiliated and and connect the presenting systems to the
screen, and all that.
And this is the setup for
the EUROSAI Congress, where there is six interpreters combinations in
all the many, on the many six, all the six
languages that the humans interpret into.
And there we are
collecting all those and, and choosing which one of them
to translate from and then translate too all the remaining
30, something languages.
So the setup is, are complicated.
This
is the workers connected to the mediator platform, and you
see that some of them are busy and some of
them are available.
So this, this is like a big
infrastructure.
So let us start with some of the issues.
Obviously the network can slow sings down.
[19:43]So once we
face the fact that in Consul where there was some
construction or whatever.
For some reason, the university had a
little slower Internet access.
And that delayed all our processing.
So immediately, we were not live.
But we were like
seconds or dozens of seconds later than the signal.
So
you you should be actually on stage or vary close
to the venue,
if you want to deliver simultaneous speech
reliably.
For test, this is not a problem.
But if
you want too do it live you you,
you should
not rely on on the Internet.
And this was the
problem.If
at the Best of Prague AI session where I
have tested everything.
It worked perfectly for the demo.
Uh,
unfortunately, even like I have asked a month shadowfax
and
then for the second time, I was not allowed to
connect my machine to a
informace connection.
I had to
rely on wi-fi.
And everything worked twice when I was
at the venue, I tested it.
Then the audience came
and the audience brought their cel phones.
Nobody actually use
the Internet.
The the cel phones were just trying to
like look around is there any internal available, and they
tried to connect.
And and maybe just these test simply
kill the wi-fi.
And it did not work for me
at all.
So nothing was ever recognized at that even
for me.
So,
so we mussed not rely on wire
wireless connection.
Then we also had issues where everything went
well, except the the final presentation was like filling endorsement
off web-browsers.
So
it was not showing properly on the
end user devices.
Everything was OK until the final presentation
point.
And this is another,
this is like
miss configuration
error like if you are elizabeths with with the setups.
It is easy to make a mistake.
So in one
of the sessions,
we were 179 some horrible
delay gaining
or growing.
And the reason was that we are in
our own ASR system, because we are also neorecormon with
that.
And we ran it locally but, accidentally,
we were
first sending the sound over wi-fi to Italy, and then
sending it bank to our place too recognize it.
And
only then we were,
we were goodlooking it.
So, this
double double network load has kill the, has caused the
delay.
Next time we, we like recognized it on the
spot
and and the the problem.If was gone.
OK.
Now,
sound acquisition.
You may recognize some of uh, one of
our concerted here.
And the problem.If is if if you
have this microphone, that chest microphone, and you put it
here.
And then you talk to the slice like that.
The microphone does not really get your ways.
So that
is, that is one problem.
This is another thing right
now,
unfortunately,
we had too switch off this chest mike
and I am relying on this mouse
mike.We want to
extend this test.
So based on three minutes of testing,
the error in 5b English and Czech is several points
lower,
if you use the mouse microphone compared to the
chest microphone.
So that is just the distance from the
mouse and the movement of the head that makes measurable
measurable difference.
So we are relying on the on the
bettor on the head microphone.
Yeah, this is a tip
for singers,
for vocalists.
You have too hold the mike
properly,
you have you have not,
you mussed not put
it in your mouse
almost.You mussed not put it to
far away from your mouth.
And also, if you stand
in front of the loudspeakers, the reverberations, will will totally
kill the signal that you are receiving through the mike.
So this is What has happened to us during the
uh, the Student Firm Fairs, when the students were were
just roaming around,
and they entered the area in front
of the loudspeakers.
And suddenly there was no way to
to recognize them. OK.
Then, there is cables.
You have
to plug the cables properly
and one of these settings
in the tampere room for the interpreters,
we were able
to get the signal
only when one of the promotes
here was not full risky in.
The reason is that
we were like misusing the connections so once once we
brought a proper sound card,
it was able to recognise
which pin of that connector has what, butt without a
proper sound card, with just the build in sound card
of standard notebook,
you have too like misalign the tavern
of that connector to get the signal.
So there was
an our or two of debugging until we've figure out
how to connect the input, so that's we're learning.
So
then there is the volume setting along the along the
pipeline.
So if you have a wireless microphone.
It has
a receiver, and there is volume control at the receiver,
and then you have a sound card, and that also
has some volume control
and it also has some button,
like two buttons.
So line level or mic level that
is one button and italia on or off.
So you
have to that's like four options, at leased just fore
the buttons, and then for the for the for the
knobs, and you have to set this
properly otherwise the
sound can be too quiet, or the sound will be
too too high,
If the the volume would be too
high and someone along the copenhagen will clip it
and
then you don't get the frequencies, and the ASR will
not work.
So you need to carefully track the signal
step by step, experienced people know.
I remember the the
colleague from the PerVoice company who came for the student
fares for the first time
and there we still were
struggling with getting the signal right.
And he like followed
from the buyer with the unprocessed always connected it cappato
and said, OK it's good here, it's good hear and
step by step,
he managed to deliver the the right
volume to the machine so that's you have to do
it step-wise, and it can fail at any point.
Yep.
So then ASR quality.
So once you have delivered the
possible the Best possible signal, What does the machine recognize
from that.
So this is an example from the from
the student, the high school students.
The speaker want to
say something like you have a bottle?. Oh, yes, we
ar situated in the heart in the heart of České
budějovice.
But in reality, this is high school student, so
he said, "you have a bottle?
Oh, yes, we are
situated in hard of České budějovice", so there was serious
mispronunciation, and there was also high level of background noise,
because that is fair,
so there is stands with music
playing,
and we are just behind a little a little
wall,
hoping too get some some shelter from from all
that noice.
So if you are unexper-, if you ar
a person and you are in this hear in this
in this harsh conditions, then you probably understand something
like
you have a bottle, because you ar not expecting the
the student to talk about a hotel on the boat,
"Oh, yes, we are situated in the heart of", and
then he would recognize that some cities being mentioned,
but
if he is from Spain, he would not know that
České budějovice, you know, is a is a is a
Czech city.
The standard ASR would deliver something like, oh,
yes, the the of the that is the the few
words the ASR can spot in the in the noise
signal
if the ASR is is noise resistant, if it
can somehow cancel the noise, then it will do something
"you have somebody to oh, yes, we are situated in
hard, which is can we do?" because it does not
know the the name of the Czech city.
And if
we have a future ASR or a person in the
in the middle actually, and then we would get the
the proper recognition of the sentence.
So this is this
is all the problems that that we are facing when
we are trying to transcribe a non-native speakers.
This is
summary across the ilo that we have made there, sow
remember, the word" error" rate for humans is around six
or four percent,
here, the Google system, Google ASR had
error rate of of ninetyish.
Edinburgh systems also, and car
systems was actually the Best one around 40, still ten
times worse than What the humans do.
And this is
this is on the ilo were all the systems have
produced any output whatsoever.
If we apply if we if
plot the same figures but we count 100 percent of
errors as the the the score.If if no output is
delivered then
Google is so much worse than Edinburgh so
Edinburgh was was better, was not like giving up sow
frequently.
And Carlos was still still the Best system, but
still the the car systems is ten times worse than
the humans
and that's then What is reported in the
literature.
Obviously, this is hard conditions.
So this is background,
noise, non-native speakers, well, we are not yet quite profession
in the recording
so we just misaligned, the mics, and
people like spoke too loud and we did not twist
the construct properly.
So it is it is vary harsh
conditions but this is the the the reality that that
you can get.
So this is the Best recording according
to the ASR quality, and there are sings like, "why
do? you wear those sow high heels"
instead of "why
do? you wear those high eat the high heels".
And
deals, there is "I know one really good star", instead
of "store", "that deals with the sale of free down
food", instead of "free time, footwear",
or something like that,
so it is it is the totally wrong.
Ok, so
What is our plan, given this.
So we are definitely
going to retrain our ASR models, and I ideally we
would like to use non native speech corpora,
common voice
by Mozilla is one of this is that that can
help.
And we have another extinguishes uh, the thing to
do.
We wood like to follow.This interpreters instead of the
floor, because each of the interpreters will sit in a
clenched with limited noise
from surroundings with better microphone. So
we have moor control of the recording, we can also
talk to the interpreters and explain to them
that they
ar being recorded and processed in particle way, and we
also have the chance to adapt to them.
So we
ar trying too get their contacts. We have still few
months to come.
And even if they are non native,
we should be able to adapt to their particular speech.
So we would ask them to do as uh,
well,
few minutes, uh possibly up to an our of of
recording we would pay them to whatever,
red some text
or or interpret something .And then we wood to create
a training dataset fore that particle person.
And then obviously,
we are going gather in domain dataset and regularly evaluate.
So does the ASR.
We get the text, so What
do? we, What do we expect now, with the translation
there are all the standard translation errors that you know
where well.
So here is one example, a sentence, which
was actually recognized perfectly.
I know which sentences are recognized
perfectly, sow I know What to avoid wen I 'm
running demo.
This is also different from the from the
users of our system, they will simply speak as they
ar used to.
If I want to showcase What our
systems is doing I will avoid named entities, I will
avoid strange constructions,
I will have like policyholder newspaper-like stile
of sentences, and then all the all the systems will
successfully recognize me.
So this is one of the the
cases.
But it is much more difficult to to ask
if you do not have any clue, and yet, the
translations, both into German,
two sample languages were wrong, the
if was translated as neorecormon or as da.
And that
is that leisure the leisure the the sense of of
that.
So my guess is that maybe in English, you
would, in in Native English, you wood actually say this
a little bit differently.
And then there might be moor
explicit clue in the source sentence. I 'm not saying
that the sentence inn English is wrong.
I 'm saying
that indeed training data, which is from news or maybe
books, or or or EU legislation.
There is this like
domain olle and even even if it is not domain
mismatch.
This is is genuinly ambiguous conjunction, and it is
difficult for the machine to guess the meaning.
So if
you are closer to the training data, then you will
see it will work better.
If if it is a
difficult, an encoded expression.
You will have these problems till
like for ever. Yeah,
here is some some just misunderstanding
or well, as the it was out of vocabulary, the
machine translation systems ran out of vocabulary:
"you can be
reported after some profanities".
So if you if you say
something bad on your website, then they will flow like
block you.
And that was the most translated as professional
things.
That is because the profanities, were not in the
in the buyer of the machine translation system.
So the
triggered to some expressions.
Okay, ah, the problem.If in spoken
language translation is that the ASR errors get kind of
worldwide in machine translation.
If you have an error in
speech recognition.
Then the error will be a few crinkled
a single word similar in shape the what I have
said.
But then you take this single word and you
translate it, and then the shape of the word like
totally changes.
So, machine translation takes all the wrong words
from the ASR as full of intolerable and happily reorders
a sentence too make it
the Best possible sentence, including
those wrong words, and there is no information about the
confidence, neither from the ASR system,
nor from the machine
translation system.
So the so the user will be just
left with the sentence, witch sounds perfectly natural love mention
some strange entities,
which were never never mention never part
of the talk, because they come they come from an
error.
So here is an example, "and the goal of
my movables is to fold", and that was "two fold".So
instead of "two fold",
the ASR boiled "to fault", that
is the natural like misrecognition. The empty output was,
"and
the goal of my theory", instead of "thesis", "is to
fall apart".
So that's definitely not What What we were
after in this system, yeah.
So this is this is
What I would probably translated too if I was lotus
it.
I just want to highlight this "two fold" is
misrecognized as "to fold" was translated as "rozdrobit se", to
fall apart.
And that is totally out of out of
the scope. Ok.
So what we have, What What is
our plan for the mission translation, data, data data, and
then maybe little bit of models.
So we are definitely
going to gather moor parallel data. University of Edinburgh is
working on that.
We are going to gather moor targets
side, monolingual data and bank translate, that's for the domain
adaptations. So this is auditing domain.
so we would like
to, uh, too focus on all these materials Uh, they
ar hardly parallel,
some of them are, but not too
many, and especially not for, uh, those, uh, like 19,
uh, non-EU languages that we are also, uh, oh, aiming
at.
So hopefully our, uh, baseline systems in the back
translation will make some sensible output from that, and statutes
we'll be able
to improved the translation quality.
We'll create
indomain test sets, and we will regularly evaluate on them.
We'll also try to, uh, uh, get, uh, in touch,
uh, with <UNKNOWN>, that's a collection of, uh, user supplied,
uh,
parallel data, and, uh, they can, given our test
sets, they can extract the most similar sentences,
so, uh,
some like filtering off off larger, uh, pool of, uh,
parallel data.
We'll also try to create gazetteers, so lists
of relevant named entities like participants, or or the precedents
of the Supreme Audit Offices.
Uh, this is, uh, having
just the list of names, uh, is not ideal for,
uh, for machine translation, but it's definitely bettor than nothing,
so we'll, we'll try this as well. I'm also considering,
and this is something that maybe someone of you could
try, I'm considering to to train machine translation on distorted
source,
uh, because I would prefer the machine translation systems
to say something in the domain and sensible, rather than
to, uh, to like, uh, uh,
try to, uh, perfectly
translate the wrong ASR output, uh,
so for me, tentatively,
I'm saying that, uh, it seems to be better to
say something similar or related, uh,
rather than, uh, uh,
like do perfect translation of the wrong input.
It, we'll
see whether this, uh, is a good assumption or not,
if if the system starts making up the content,
then
obviously, the users will not be happy as well, but,
uh, right now, uch, uch, uch,
our systems is too
easy to, to, like, uh, to, to, to, to, um,
that, to, it is too easy to too notice that
our systems is doing something wrong,
and, uh, we want
to hide it a, uh, uh, bit under the carpet,
so let's see if this will be a good strategy
or not and let's see if we manage to train
such a, uh, model.
And I'm afraid that we will
never get, during this few months, uh, to the interesting
sings like the speaker's gender.
Uh, obviously, uh, the machine
translation systems has no information about the gender of the
speech.
Uh, the parallel data, uh, is totally like, uh,
non-labelled, uoouh,
it doesn't know about that fact either,
and,
uh, uh, English doesn't, uh, marque most sentences for the
gender of the speaker, uh,
so it's vary likely that
our systems will be just making up and switching the
gender of the, of the speaker, uh, in the first
person sentences, uh,
and that will be vary confusing for
the audience.
So, uh, I'm afraid that we will not
get to this, unless there is someone who would like
to help. Okay.
Then the integration of the ASR and
MT.Uh, I've already said, uh, ASR 18302003 strings of
realms
words, uh, machine translation expects individual correct sentences,
so there're
a number of options that we can try to, uh,
uh, bridge the gap.
So we can try to hurts
punctuation into the ASR output, uh, which we caul the
segmenter, uh, or, uh, we can change the ASR, uh,
to arts directly
correct punctuation, sow the speech recognition would
run not into, uh, just sequence of words, butt sequence
of words with punctuation and
capitalization. And a student of
mine is is working on this. And we can also
try something which is one of the research goals of
our project, uh,
that's full end to end spoken language
translation, uh, with one ionising systems that, uh, gets the
sound in one language and produces, uh,
the translation in
the, uh, in the targets language right away, so that's
again part of, uh, Peter's, uh, thesis.
So the segmenter,
given a stream of words, guess punctuation, casing, we use,
uh, uh, Ottokar Tilk's tool, so, someone's tool.
Unfortunately, the
speech information is no longer accessible to this tool, sow
it has just the sequence of words, and based on
the sequence of words,
it is fudge sentence boundaries. And
as, uh, one of, uh, my colleagues, uh, from the
<UNKNOWN> causing said, uh, it's actually doing vary good job
given
that it doesn't have any information about the pauses,
about the intonation, sow it's surprising how well it can
arts the sentence boundaries
with this, uh, poor information. Uh,
so, still, it is it is far from perfect, so
there're many errors.
Uh, there is also another tool, which
would consider the strict and and delays, but we haven't
had time to integrated it,
so if someone would like
to, uh, play around with that we will be happy
to, uh, uh, to, uh, to get any help there.
So, uh, <UNKNOWN> has trained this, uh, this, uh, segmenter
tool, on, uh, <UNKNOWN> data, about four million sentences,
and,
uh, here is the precisions and recalls for various punctuation
marks that we are inserting, uh, and h'am, uh, highlighting
the recall of, uh,
period, because that is critical to
to identify sentence boundaries, uh, so, uh, it's, uh, 70
to to 80 percent, which is reasonable, uh, uh,
if
you look at it as a number, but it is
still far from sufficient for, uh, practical, uh, use.
Uh,
okay. So, uh, hear is an example, uh, of an
error in, uh, in, uh, in the, uh, in the,
uh, in the segmentation, so errors in precision, uh, lead
to fudge empty output,
so here the speaker said something
like "all too well", uh, that was part of a
longer, uh, uh, longer, uh, phrase,
and, uh, the ASR
plus segmenter, uh, putt a full stop between this, like
in the middle of this phrase, uh, leading to the
output
"This approach does not generalize all too", and then
separate sentence, "Well, sow to somehow conclude that the whole
talk",
so here the trio is obviously changed, eh, quite
a bit, uh, this is exactly the case where the
machine translation would, uh,one by one received the first sentence,
and then the second sentence, and it wood have no
way to recover from, uh, from the full stop, which
should not be there in the first place.
So, uh,
there the two sentences would get, uh, like, beautiful, uh,
but, uh, the, yeah, the message wood be distorted. Errors
in recall, uh,
make, uh, too much content unstable, and
we'll explain this, uh, on on on the following slides.
Uh, yeah.
So, here is another, uh, technical thing, uh,
the the bounce between the speech recognition and and spoken
English translation.
Uh, so it's, if it is the online
speech recognition, uh, we are receiving text messages, uh, but
these text messages of strings of words
ar not related
to sentence boundaries. Uh, sow in the first run, and
this is What we, uh, What we were mostly seeing
in in March, or at all
the sessions, uh, that
were the test fore for the Supreme Audit Office, we
were receiving, uh, these messages, so
this is the first
message, the second message, third message and fourth message, and
we were directly sending these messages, uh,
to the machine
translation, um, so, uh, the first message was "You should
think there have", and there, that's still unfinished sentence.
Uh,
machine translation system, given this input, decided to totally 8°c
the first sentence, because the first sentence, like,
doesn't fit
there, the machine translation systems is trained to translate full
sentence into full sentence, sow some half a sentence, uh,
here, let let's drop it, so it said just, just
"Děkuji", že "Thank you". And then, uh, the, uh, this
is an extension of that message,
so the ASR sent
again, uh, the same prefix, and then it added the
extra words that came in the meantime.
Uh, this is
probably the Best translation that the MT could have delivered
for that, and then the ASR sent something which we
call
the confirmation message, sow the ASR says, well, "I
will not bother with these three words anymore, 'You should
thank', I'll just, uh,
I'm rash finalizing these, and from
now on, I will only send you the rest, 'There
have been many revolutions'".
So here is a single message,
"You should thank", sow that's one complete sentence, and a
beginning of another sentence, and we have
translated it as
as if it was one sentence, and that's that's just
wrong, and then, uh, the next message stars in the
middle of the sentence already,
and we are again feeding
it as if it was a complete sentence, so there
is, uh, like, the sentence boundaries are totally unobserved by
our approach,
and that that eu27 everything, it it emits,
uh, it it upgrades all of these partial sentences into
full sentences, butt that's not related too the,
uh, uh,
to What the, uh, to What the people said. Okay,
so if you put together, uh, the, uh, the ASR
and MT better, and we did it and I'll show
you on the following slides,
then you still need to
present somehow the result, sow that's we're slowly coming towards
the the final stage, and still,
it can totally kill
the the show. Uh. Sss. If you have a font
too small, such as in the subtitles, if they were
a little bit smaller, it,
they will not be of
any use for you at all. So you need to
vary carefully balance like how much content you put somewhere,
and if the font would be, uh, legible, if it
still, if it's eligible fore people in the first row,
and for the people in the last row,
if they
have to rely on the cel phone and suddenly they
have a different shape, a different rectangular area where where
the, uh, translations
and transcriptions and appear, and this heavily
affects how much information we can send them, and we
have to leniency decide What What to sent them
and
and what not. Uh, for example then, if if if
this text řízení too much, if the words change too
quickly, you cannot red them.
Uh, so that is one
of the reasons why we have not employed, uh, the
fully, uh, byla network ASR systems so far, because they
ar trained
to operate on window that moves in time.
Uh, so, uh, eight seconds, uh, at one go ar
always recognized, and the beginning can change,
so, uh, imagine
eight seconds of my speech back, and still like redeciding
What is the first word. As mostly the words do
not change,
but sometimes they do, and if you have
this long piece of text, and it, like, systolic here
and there, uh, there and bank on on various, uh,
places
in in various words, you do not know how
to follow.This that, it's it's like unstable text. So we
ar working on this, uh, on this integration.
And, uh,
I want too highlight, that this presentation mussed be tested
on stage, you cannot test it in simulation, like, uh,
the the ecj of the text, the visibility from various
areas, that all can, uh, um, kill it, can inefficient
affect it in such a way that
it's not usable
at all. If you have ever order something from an
e-shop, and it came, but it was like the wrong
size, either too small or too big,
then you know
What I'm talking about. If, in in an e-shop you
have a picture of What you're buying, butt the picture
is out of proportion,
you you don't know if you're
getting this big teddy bear, or or this tiny teddy
bear, uh, so it's it can be of wrong size,
and that's the same thing as, uh, hear for, uh,
for the subtitles. So hear is, uh, one, uh, of
the views that, uh, you have been,
maybe you have
been watching on your machines. Uh, this is mainly for
showcasing that we can run the translation into many targets
languages, uh,
so we are showing two lines of subtitles,
uh, in, uh, ten languages at the moment, and this
will be 40, uh, 43.
Uh, this is good to
demonstrate that the system is following me, and it can
create, uh, fall positive impression, uh, because
if you're following
me, and if you understand my my language, then, uh,
you cannot, you can no longer red carefully What is
in the in the subtitles,
and you will only observed
the right threatens there, so if I say something about
subtitles, and you will see "titulky" in Czech then you
will notice,
oh, great, the system is boiled this person,
but you are not able to judge the quality of
the sentences.
So the sentences, in fact, are pretty crappy,
and they jump too quickly, and if you, if you
did not understand me and you only had these two
lines to follow,
then you would get lost in the
errors of ASR and MT. So this is this is
good to show off, but it's bad for the user.
So we have something else, we also show, uh, much
longer context, uh, so, uh, the, the text as it
grows. And here, uh, you have, uh, I don't know
if this was probably given in English,
and then it
was translated into, uh, Czech, uh, and German, uh, and
here you already see the differs between the stable output,
the partial output
and the, like, the incoming output. Uh,
so, some of the sentences are fully processed by all
the pipeline, all the components in the pipeline,
and there
is no way they could be updated any longer. And
these sentences are shown in, uh, are shown in black,
uh, and then, uh, there're sentences,
which are, uh, gradually,
uh, being updated, and here still the punctuation can, uh,
can like make a difference, so that's why we're only
showing this only in gray.
And then there is the
last sentence, which is still getting more and moor words.And
as the segmenter io processing of some of this
then
the full some full stop here will appear. And it
will be out of the uh, the processing area of
of the segmenter anymore, and it will become
the black
and full stable output. Here you see why the full
stop recall is important. If the full stop are too
international they are too precludes
in the output, then to
much of text will remain in this gray fase and
too much of text will like be still advantageous and
flickering.
People will wait wait too long for the stable
output. And you also that it is also like matter
of taste, whether people would like to follow.This the more
the moor unstable moor recent output, or weather they would
like to follow.This only the stable one, butt be several
seconds behind the speaker.
So that that differs. So this
is something that we have to update. And ideally, it
would be the users choice.
Would you like to have
the the the endorsement possible delay at the risk of
the reading something which will be still like fixed,
or
would you like to wait little longer until our output
is stable.So that is something that the the trader the
the user would choose.
Yeah, so if people are provided
with this longer context, then they can better recover from
errors along the pipeline.
So it is almost possible to
follow.This the content of a talk which is not known
to you if it is not too far from the
domain of the training of the machine translation system.
Ah,
okay, yeah.So here is a little summary of what has
to be done.If you are presenting text only in the
two lines of subtitling.
So you have some ASRs.So this
is some English stream of words.Pixels on your screen.At any
given moment.It is also vary flexible
architecture of.This is an
entire book. So you see that there is no sentence
boundaries.And the segmenter needs to predict some sentence boundaries.
So
it predicted a full stop here and a full stop
here.And then some ASR update comes.And it also, based on
this update, the segmenter has
another update.So it shadowfax one
moor full stop here. So this is like bug of
updates coming.And some of these sentences are already
completed.So the
segmenter says:"This full stop is final. I will never never
take it bank Do with this sentence, whatever you like,
I will not touch it."Then
some of these some of
these full stops are still unstable.So I may will remove
this full stop, and I may move it slightly.And then
we also have
to like handle that properly.And then yes.And
the rest, the last sentence is only expected or incoming
like we do not know What What the sentence
will
finish like.We are sending now full sentences to the machine
translation system.So that is the the that is like good.
That means that the empty systems fed with the input
is ready for.So now, growing into German.It will say like
something like:
"Pixeln antidiabetic Ihrem Bildschirm Zu jedem Zaitpunkt.Es 18302003
auch eine sehr flexibele Architectur.
Also, so this makes this
makes sense. This is like correct German sentences.The issue is
how much space you have to show this German to
the user.
So if we have two lines of subtitles,
then we would then we would show this yellow part
and this yellow part.And then, because the last sentence got
some extra words, brennan goth extended, the systems decided to
change the beginning of of this sentence.It decided to change
the last word
on the first wakes line.If the line
is like still visible we can easily do the change.And
nothing bad happens.If we are limited to just one line
of marshes
output, then this line has been already rolled
out.Like the the the the user can no longer see
it.The user saw it a second ago.But now this is
no
longer accessible.So we would have to show the like
only half of the sentence, and that is something that
we cannot do.So the sentence has changed.
We call this
reset.Now the now these outlived mechanism has to go back
and show something which has already trio up and show
it again
so that the user can reread. But this
reset is extremely annoying.So you may have noticed that sometimes
this these subtitles goth
red.And the the red effect is
exactly highlighting a reset has happened.So the the segmenter the
segmenter was oscillating like where
to put some of the
full stops, where to put the ah, the inhibits changes.And
if the change of the full stop or the inhibits
happened above the edge, in
the line which already rolled
up, then the subtitle, even for here for ASR issued
a rested, and it also indicated in red.So the red
for me
was like a remark.Yes, this is the red.So
this is the this is the case where the subtitles
had to goblins back, because something was updated outside of
the screen.
So that, that is an annum why the
limited output space is sow critical for delivering the the
translation too the user.
If you limit yourself too much,
if the language differs is too big, and sow the
word reordering and the length of the sentences of like
make you
beams larger bits that then fit on the
final screen, then the user will be experiencing vary fudge
resets.So I 'm like stressing this because this
is something
this is these are decisions that you make along the
whole pipeline.And it is only the vary last bit the
size of the window that eu27 it,
or or makes
it make it workable.So that is that is why we
ar like fighting in the consortium.Like What is the better
output.The it seems, based on
these arguments, that you definitely
do? not want to have these subtitles.You you prefer you
you should prefer the longer paragraph view that that you
saw on the previous slide.But the problem.If hear is that
if you if you make some vary bad mistake, if
some some bad word of frmo profanity appears
in the
output, it will stick there for too long.And people like
the take out their cel phones and and take pictures
of that of that bad word.
So ah, you ar
druid too much of of your problems in this setup.So
that is why that is why the the some some
partners in the consortium preferred
the the subtitles.But there is
a big risk of of the subtitles being incomprehensible.Ok.So then
there is the overall customary load, and the the the
overall usability.We have two users, they are here, who confirmed
that.So sow sorry.There is many that that is another these
ar these here.
So there is sow many users who
have tried too follow.This this have said that there is
no way that you would be following the translation on
your notebook
and looking at the slides or the speaker,
who is in front of you.Even if it is like
in a vary close angle, if you do not have
to move your eyes too much,
you still have to
accomodate because of the different distance, and, that will take
some time.And then you will, as soon as you look
up at the slides,
you will loos the subtitles, and
you will loos content.So we are now working on adding
the slides also to the to the screen where the
subtitles
appear, or the another way around putting subtitles to
the to the screen, where the slides are.So this is
something which which has to be done.
And then the
overall usability.The overall usability is still vary bad.I'm intervenes you
whoever does not speak French, and wood like to join
us.
We are, hopefully this week, we are running like
a French movie watching session, or just ten minutes of
a TED talk.We do not speak French.
So we will
follow.This our systems, boiled the French and hasty into English,
or or check or any another language.And it will try
to come up
with improvements that would make it actually
usable.Because it is vary big difference, weather you can understand
the source language, or or not.
Our, we had a
similar session with Martin Popel presenting something on machine translation
evaluation in Czech and it was addended by our two
foreign students.And these two foreign students reported that they could
follow.This the Czeck talk, if they fully focused on the
paragraph view. And then
as soon as they looked upon
the slides, they goth lost.So if with full attention to
the slides, it was possible to follow.This it.
But this
is not for normal users.Like normally users have to have
to have some free time for their for their brains
as well.
I wood like to highlight that the desired
setting differs from user to user.Those who understand the source
language will need the simultaneitly,
and they will prefer it
over precision and stability.If you are following what I 'm
saying, then and if you are reading the subtitles in
your mother
tongue, then they could provide you with the
words that you missed.If you do not understand a word
it could appear here, you would understand it.
But it
has to appear here at the like on the spot,
at at the moment, when I said the word.If it
is there three seconds after, then you, it was of
no
use to you anymore.Because yeah like you have moved
in what you are listening already.So if you are following
the source language, the subtitles in
your mother tongue can
help you, butt they have to be immediate.If you do
not understand the source language, then you are only relying
on the text.
And in that case, you would prefer
stablility and precision And you are happy too wait fore
seconds. You you do not know What I 'm talking
about at all.
So it is not a problem.If if
if you get my message eight seconds later. So this
is two vary different use cases. And it should be
the user to select
which which of the authentic is
is there not not us.OK so evaluation.There are three aspects
of spoken language translation.Quality, that is something
that we know
from machine translation.And we know how too have 233 estimate
the translation quality.But there is also the lag.
How much
is the text in the translation delayed behind the source.Some
of the isaac is obviously inevitable, because you have to
wait for the German
verb, or we have to anticipate
it in some way.And there is flicker.So if you have
a systems which is updating, and we have such systems,
then these
systems can anticipate, and they have the capacity
to coring themselves.So then suddenly they can create some output,
and then remove it in the next
step.And this has
to be controlled vary much so that the user is
not confused.So we are working on an evolution tool with
E brahim Ansari.And hear is just the the the most
serious problem.
If you have this English ASR.Do you know
it is a cat.Isn't it.Yes it is like that.So let
us imagine that the segmenter has overriding it into these
two chunks.There is no way to hoarse it well with
this German translation reference,because it has three sentences that they
the words are whirled in
a different way in the
sentences.So we have to somehow force align these sow that
the basic set of units that we are evaluating over
is common too all systems, regardless of what the English
ASR does.Uh, everybody has to reach the
same reference segmentation.So
there is some strategies.I'm not going into the into the
details, any more.Essentially, we have either the option to
work
with these sentences, like consider moor sentences at the same
time, or we can totally forget sentences, and we can
evaluate by evaluating
30 seconds chunks.So did the the MT,
the SLT pipeline, the SLT pipeline produced the right words
within these 30 seconds?
And if it did then, yes,
if not then not.So this is this is like a
way to go around the problems of sentences not being
customary in in speech
at all.Yeah.So the surge, or the
mobilization, as I as I say it.The battle towards a
usable systems is still not lost.So we are now building
of an
army of paid volunteers.And if you have any
colleagues or students who can help us between now and
May or June, we are totally happy too take them
on board.We are meeting at least once a weak for
just 30 minutes and virtue who is working on what.And
we have funds for this.
So this is like, ehm,
a paid work And you can do many smallish tasks.For
example, if you are looking at some of the MT
outputs,
there could have been bad characters, because no one
has yet ctrlc checked where they come from in the
pipeline.Then there is larger
sings that we need to develop.We
would like to have a a dashboard, which we will
give a configuration like a life systems running, which cutoff
logs, log files and this filtering should allow us to
immediately see where is a problem.
So for example, if
we are 5b the English clenched with the French ASR,
then there will be many French words coming from the
systems, but they
will be totally wrong.That is the the
most laughable output I have seen so far. Like mono
the languages for the ASR because the the systems
will
struggle.It will be like words similar with pronunciation, butt in
the wrong language, and the sentences won't make any sense
whatsoever.
So this can easily happen.If you are following six
inputs and hasty into 43 languages, you need a harming
view to too to know What your systems
ar doing.So
that wood be a dashboard.Then a lot of work on
the domain and speaker adaptation.Just just getting in touch with
all the interpreters those who will be in the booth
and conducting them, asking them to come hear record an
hour each.
And then using this data to adapt the
system.Again.A lot of a lot of little work.Code switching is
something which I have not discussed
in my talk at
all.But I'm angola how frequent it is, not only in
linguistic lectures, where we have examples in foreign languages
But
also native speakers of another languages than English frequently bring
in English phrases, and the ASR systems are totally not
ready for that.
So then the, it is vary similar
to to the named expression or named entities.It is simply
unknown words that get recognized into something very, vary bad.
And then when this gets translated, it is vary vary
laughable.There is also another activity, which wood really like to
start call in leather
climbing.So it is wen you have
fixed test set, and you are working on improvements of
your model, er, or the pipeline.So you're making many small
changes.
And you are always evaluating on the test set.So
you are climbing the leather of performance.So this is this
is What we have to run.
I'm hoping for like
two bigger model retrainings until until May and many optruma
changes, many like smaller fixes that can improved
the score.If
quite a bit.And.Yep.I've already talked about this this dashboard.But this
is still something different.The dashborad is like technical, uh, set
up.
You want too know that the sound volume from
all the six inputs is the right.With this multi-source monitor,
you assume that the,
technically, the architecture is all set
up well, butt still, you need to decide which of
the ASR in the combination with the speaker works best,
and which of these sources com2004 the Best translation quality
into the many targets languages.So this is like a technical
check that everything is
running. And this is selection of
the Best path through the, for the Best output quality.Yeah.To
summarize. ELITR subtitling, er, a is a big
challenge for
the project Even if you connect too movables components, they
can still deliver crap together.
So we are now working
on making this usable.Technically, the complex statute now works.It has
been working...How many restart did you have to, er, do,
Sangeet? Multiple?
Yeah.Okay.So it is not seamless.But with one operator,
it can survive 60 minutes of of speech, and...But unfortunately,
even with this whole systems working, the benefit for the
end user is still limited or nonexistent.
So.If you did
not follow.This my English speech, if you only had a
chance to red the Czech translation, I'm curious how much
you would get.
So this is one of the...We are...We
have still a few months to go.So I'm I'm vary
lucky that that we have these months too to fix
the pipeline.
And some of the problems that we are
running into are not going to be solved in the
next two, three, five years.
But some of them can
be fixed So we will do our Best to to
make the the the output actually usable.
And yeah.Please.Please join
us.
Ah, because ah, yeah.With.That's it.So we have a technology,
which technically works butt it is totally unusable.
And and
I'm curious, how how far we get in in these
months.So we are running this this army or this this
mobilization.
So we seek fore your help and just talk
to any of us.So maybe those of you who ar
already on the surge team, just raise your hands
so
that people see One, two, three.Ah.Yeah.Atul.So at least four fore
people in the room already.And if you have more...Or
if
you have students who would like to get in touch
with this.This is a nice -- it is not summer
internship -- it is like spring, spring internship.
Please, please,
let they know.And you can red all the details about
the project in the blog.And if you want the the
real truth, then talk to us
directly.Yeah, OK.Thank you.Thank you
vary much for...And I think we still have some minutes
to ask questions or ad some details.Yeah.Hanka.
Well, you spoke
about the the the problem.If from ASR.If there is a
mistake in understanding of the of the word then it
brings the problem.If
further to the machine translation.And you also
spoke about the the module, that puts the punctuation into
into the steam of words.
Did you consider some something,
like rattled some sort of distributor checker, or some language
model that would check whether the supposed sentences
ar really
sentences or dramatically good enough to be sent to machine
translation.So yes, and no.The whole punctuation insertion,
the segmenter is
a language model as such.So that is that is just
like n-gram-based.But it is not n-gram.It is it is longer
sequences.
It is a neural-network-based language model.And it is trained
on concatenated sentences.But I think these sentences are concatenated
from
already a shuffled training set.So it has...Like....The sentences came became
adjacent just by randomness, and not by a proper context.
So there is a certain limitation.So that is that is
the yes part of of that.So it totally ek on
some language model inn the statistical
sense more.And some grammaticality
of the sentences?No, no.We are not considering that at all,
because, well, we also do not assume that people
would
utter grammatically correct sentences.So that is the um the the,
problem, or the task that you may be referring to
would be called speech
reconstruction.And this is something that that
our Department has been working on in the past as
well.There the goal is to take the partly
disfluency speech
of of myself and convert it into a written-like speech.Like
proper all, only all correct, grammatically proper sentences.
But we
ar not working with with this component at all in
in our system.So quite on the contrary, we are rather
moving towards machine
translation systems, which will be robust to
carlson even these a partial sentences.So, I have not mention
in the slides, butt another of
students of mine, Dominik
Macháček, is working on on machine translation, which runs not
on sentences, butt on a window of words.
So we
would like to get rid of the segmenter altogether.And we
would, hopefully, if if we're provided with good training data
that has this
property, then the translation systems would be
able to translate disfluent sentences into disfluent sentences mustang in
the targets language.
More questions?OK.I have the question about the
issue of where to place your microphone, because several times,
I saw some speakers holding their microphone
under their chin,
like this.Have you ever seen it?It's not frequent at all
that they're claiming that it's it's...You can...
It is completely
understandable.And you prevent flowing the air.We have not tried.So next
next time you see me giving a talk,
I'll be
surely holding a microphone under my chin too evaluate it
properly.I realize that there is no loud speakers here in
the room.
But when you put the microphone under your
chin your voice has changed here in like in the
in the loud in the in the wild.Yeah.
So I
do? not know, maybe maybe the recording has not has
not seen any change, but we have.OK.I have two small
questions.One question concerns or,
it is a remark.That I think
that if strict is paid due attention to, then it
might help for the storms a lot.
Of course, in
the policyholder speech, people don't behave according too the rules.That
still.But I have one question for your slide number 43.
I just wondered.Yeah.There are between...In the 34 part you have
waking but you don't have punctuation.I think this is bugs.Ok.
So, so we do not know.I do not know...We can
actually give it a try.If I launch this.But where where
is this that?So.So here.tiny So I will I will...
I
will stop the live subtitles, because it is picking...Or I'll
leave or on.I am pretty happy with your answer.So.So I'm
I'm curious myself because hear is the where is that....
Because I have one more general question.Yeah.Yeah.So please, feel free
to ask.I will just go to the paragraph view in
the meantime.
So let us switch off the Hindi and
Hungarian.And and X1 and X0.So this is my English recognized,
and it is being translated into German, and into Czech.
Yeah.And and the more general question, which may be you
have heard seven times.How much can you learn or ad
to your considerations if you ask the translators...
Interpreters...Interpreters...Interpreters who
have the simultaneous translation...From their strategies or something like that...
Is there something in the in the system, which would
reflect this experience?Not yet.
Not at all.We are in touch
with them, because you like it, it helps us.
So
we, we...Thanks too they we can get hold of the
data, but their interpretation strategies are much moor much moor
long-range, so, to say.
So, they are happy to listen
to a couple of sentences, even for the simultaneous translation
So simultaneous can still mean 20 seconds after that.
And
they will first get the whole idea.And they, they will
speak it from scratch.
And this is vary distant from
What we are doing, but we are aiming to get
into the same realm, sow to say.
So we have
now our vary first data sets where we have the
source speech in English, which is being interpreted live into
Czech by some of these students.
And so we are
also transcribing the English.And we are confiscation in the text
form what was said in English.
And the evaluation tool
that I mention that will consider the quality in terms
of BLEU score.If or a similar score.If for machine translation,
the isaac and the flicker, is meant -- it is
not yet tested -- but it is meant to be
applicable also too human interpreters.
So ideally, we would see
that the human interpreters are much better in precision, or
the the quality of the output, but they will be
much slower.
They will have much longer lags.And they will
obviously have no lotus or vary likely they will...Well, it
will be difficult to spot the flickers, because the the
the way will be collecting the data.
So.But we would
really like to have automatic tool that c2004 both SLT
systems and interpreters inn in the same three scales.
And
then we see What are the complimentary benefits of of
these like groups of of processors.
Their strategies so far
ar are not of any use too us.
And also
their evaluation strategies are vary distant from what we ar
like talking about.
So that that...Yeah.We have seen a revolver
of when they are being graded, the interpreters.And and..Yeah Some
of these items are are totally irrelevant to our technology.
Ok. Thank you.
So I am afraid we have to
stop at this point.
---
So think you again for
your talk.
And I just would like to shaithis that
we don't have a session next week.
And you will
learn about the title, and the speaker of the next
following weak in your emails.
So think you vary much.
Thank you.
