{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import math\n",
    "from time import sleep\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_homophone():\n",
    "    begin = \"https://www.homophone.com/search?page=\" \n",
    "    end = \"&q&type=begin\"\n",
    "    lines = []\n",
    "\n",
    "    for i in range(1,87):\n",
    "\n",
    "        # create url & obrain request\n",
    "        url = begin + str(i) + end\n",
    "        print(url)\n",
    "        result = requests.get(url)\n",
    "        beauty_result = BeautifulSoup(result.text)\n",
    "\n",
    "        # parse request\n",
    "        list_of_cards = beauty_result.find_all('div', attrs={'class':'card'}) # ok safe now\n",
    "\n",
    "        for card in list_of_cards:\n",
    "            line = []\n",
    "            words = card.find_all('a', attrs={'class':'btn word-btn'})\n",
    "            for word in words:\n",
    "                line.append(word.getText())\n",
    "            if len(line) >=2:\n",
    "                lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, data):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for line in data: # for each set of homonyms\n",
    "            to_write = \"\"\n",
    "            # check that there is at least 2 homonyms...\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            # create one line    \n",
    "            for i, word in enumerate(line): \n",
    "                to_write += word\n",
    "                if i+1 < len(line):\n",
    "                    to_write += \"/\"\n",
    "            to_write += '\\n'\n",
    "            # save line to file\n",
    "            file.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# remove phonetic translation ....\n",
    "def strip_between_brackets(line):\n",
    "    r = line.strip()\n",
    "    regex = re.compile(\".(?![^\\(]*\\))\") # skip between  ()  ... but not working as expected ... ')' remains\n",
    "    result = re.findall(regex, r)\n",
    "    r = \"\".join(result)\n",
    "    r= r.replace(\")\", \"\")  # and is solved here\n",
    "    \n",
    "    rm1 = '\\['\n",
    "    rm2 = '\\]'\n",
    "    regex = re.compile(\".(?![^\" + rm1 + \"]*\" + rm2 + \")\")  # are [_doublet_] \n",
    "    result = re.findall(regex, r)\n",
    "    r = \"\".join(result)\n",
    "    r= r.replace(\"]\", \"\")  # and is solved here\n",
    "    \n",
    "    r = r.strip()\n",
    "    r = r.strip(\".\")\n",
    "    r = r.strip()\n",
    "    return r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all files ... one by one...\n",
    "path = \"scraped-homonyms/8-oronyms.txt\"\n",
    "\n",
    "w_lines = []\n",
    "\n",
    "tmp = []\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    for line in file:\n",
    "        \n",
    "        line = line.strip() #strip_between_brackets(line)\n",
    "        \n",
    "        if \"Stressed vowel\" in line:\n",
    "            continue\n",
    "        if \"Lexical set\" in line:\n",
    "            continue\n",
    "        if \"Sound:\" in line:\n",
    "            continue\n",
    "        if \"See also\" in line:\n",
    "            continue   \n",
    "        if \"Nonte:\" in line:\n",
    "            continue \n",
    "        if \" split\" in line:\n",
    "            continue\n",
    "            \n",
    "#         line = line.strip()\n",
    "        if line.startswith(\"http:\"):\n",
    "            continue\n",
    "        if line.startswith(\"https:\"):\n",
    "            continue                \n",
    "        else: # line == 1\n",
    "            tmp.append(line)\n",
    "            \n",
    "        words = line.split(\"\\t\")\n",
    "        w_line = []\n",
    "        for word in words:\n",
    "            word = word.strip()\n",
    "            if len(word) > 0:\n",
    "                w_line.append(word)\n",
    "        if len(w_line) > 1:    \n",
    "            w_lines.append(w_line) \n",
    "            \n",
    "            \n",
    "# and write it\n",
    "write_to_file(\"scraped-homonyms/8.txt\",w_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Merge all files to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped-homonyms/1.txt\n",
      "scraped-homonyms/2.txt\n",
      "scraped-homonyms/3.txt\n",
      "scraped-homonyms/4.txt\n",
      "scraped-homonyms/5.txt\n",
      "scraped-homonyms/6.txt\n",
      "scraped-homonyms/7.txt\n",
      "scraped-homonyms/8.txt\n"
     ]
    }
   ],
   "source": [
    "# create list of sets\n",
    "list_of_sets = []\n",
    "\n",
    "# read file by file\n",
    "for i in range(1,9): # files 1..8\n",
    "    fname = \"scraped-homonyms/\"+str(i)+\".txt\"\n",
    "    print(fname)\n",
    "    with open(fname, \"r\") as file: # for each line\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            line = line.split('/')\n",
    "            spliteed_set = set(line) # read homonym group\n",
    "            \n",
    "            added = False\n",
    "            for ss in list_of_sets:\n",
    "                if ss.intersection(spliteed_set):\n",
    "                    added = True\n",
    "                    ss = ss.union(spliteed_set)\n",
    "            if not added:\n",
    "                list_of_sets.append(spliteed_set)\n",
    "\n",
    "# writing part\n",
    "write_to_file(\"homonyms-data/scraped-merged.txt\", list_of_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
