{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def write_to_file(filename, data):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for line in data: # for each set of homonyms\n",
    "            to_write = \"\"\n",
    "            # check that there is at least 2 homonyms...\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            # create one line    \n",
    "            for i, word in enumerate(line): \n",
    "                to_write += word\n",
    "                if i+1 < len(line):\n",
    "                    to_write += \"/\"\n",
    "            to_write += '\\n'\n",
    "            # save line to file\n",
    "            file.write(to_write)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def parse_giza_output(source_file, target_file):\n",
    "    # Sentence pair (1) source length 4 target length 4 alignment score : 0.0999093\n",
    "    # This is fully overloaded. \n",
    "    # NULL ({ }) This ({ 1 }) is ({ 2 }) fully ({ 3 }) overloaded. ({ 4 }) \n",
    "\n",
    "    list_of_sets = []\n",
    "\n",
    "    # parse giza++ output\n",
    "    with open(source_file, 'r') as file:\n",
    "        print(\"parsing source file: {} ... \".format(source_file), end=\"\")\n",
    "\n",
    "        while True:\n",
    "            # for each aligned lines ....\n",
    "            lines_gen = islice(file, 3) \n",
    "\n",
    "            target_sentence = None # ASR [N]\n",
    "            source_sentence = None # original [1]\n",
    "            for j, line in enumerate(lines_gen):\n",
    "                if j == 1:\n",
    "                    target_sentence = line\n",
    "                if j == 2:\n",
    "                    source_sentence = line\n",
    "\n",
    "            # check that we are not at the end of the document ...\n",
    "            if not target_sentence or not source_sentence:\n",
    "                break\n",
    "\n",
    "            # split target and source\n",
    "            target_list = target_sentence.strip().split()\n",
    "            source_list = source_sentence.strip().strip(\"})\").strip().split(\" })\")\n",
    "\n",
    "            # for each of aligned words in source\n",
    "            for align in source_list:\n",
    "                source_word, idx = align.split(\"({\")\n",
    "                # strip some dirt from source word\n",
    "                source_word = source_word.strip().strip(\".\").strip().strip(\",\").strip().strip(\"\\\"\").strip().strip(\",\").strip()\n",
    "\n",
    "                # skip NULL alignment or UNKNOWN words in source sentence ...\n",
    "                if  source_word == \"NULL\" or source_word == \"<UNKNOWN>\" :\n",
    "                    continue\n",
    "\n",
    "                # and aslo skip unalgined wotrds ... ({ })\n",
    "                idx = idx.strip()\n",
    "                if len(idx) == 0: \n",
    "                    continue\n",
    "\n",
    "                 # 1 -> N alignment    ({ 2 5 6 })\n",
    "                if len(idx.split()) > 1:\n",
    "\n",
    "                    # build target word (N-gram)\n",
    "                    target_word = \"\" \n",
    "                    list_idxs = idx.split()\n",
    "                    use = True\n",
    "                    for idxx in list_idxs:\n",
    "                        assert idxx.isdigit(), \"align idx must be integer !\"\n",
    "                        idxx = int(idxx) - 1 # idx from zero ..\n",
    "\n",
    "                        # do not add space if not suitable ...  for example: \"I'll\", \"zero-based\" ... \n",
    "                        if target_list[idxx][0] != \"'\" and target_list[idxx][0] != \"-\": \n",
    "                            target_word += \" \"\n",
    "\n",
    "                        # do not use some strange alignments ... for example\"  hello / The.Hello and\n",
    "                        if source_word == target_list[idxx].strip().strip(\".\").strip().strip(\",\").strip().strip(\"\\\"\").strip().strip(\",\").strip():\n",
    "                            use = False\n",
    "                            break\n",
    "\n",
    "                        target_word += target_list[idxx]\n",
    "\n",
    "                    # strip some dirt from target N-gram\n",
    "                    target_word = target_word.strip().strip(\".\").strip().strip(\",\").strip().strip(\"\\\"\").strip().strip(\",\").strip()\n",
    "                    if not use:\n",
    "                        continue\n",
    "\n",
    "                else: # 1 - 1 alignment  ({ 4 })\n",
    "                    assert idx.isdigit(), \"align idx must be integer !\"\n",
    "                    idx = int(idx) - 1  # idx from zero ..\n",
    "\n",
    "                    # strip some dirt from target word\n",
    "                    target_word = target_list[idx].strip().strip(\".\").strip().strip(\",\").strip().strip(\"\\\"\").strip().strip(\",\").strip()\n",
    "\n",
    "                    # only different words are intereting...\n",
    "                    if source_word == target_word:\n",
    "                        continue\n",
    "\n",
    "                # print(\"{} --> {}\".format(source_word, target_word))\n",
    "\n",
    "                spliteed_set = set([source_word, target_word]) # read homonym group\n",
    "\n",
    "                added = False\n",
    "                for ss in list_of_sets:\n",
    "                    if ss.intersection(spliteed_set):\n",
    "                        added = True\n",
    "                        ss = ss.union(spliteed_set)\n",
    "                if not added:\n",
    "                    list_of_sets.append(spliteed_set)    \n",
    "\n",
    "    print(\"OK\\nsaving result to: {} ... \".format(target_file), end=\"\")\n",
    "    # writing part\n",
    "    write_to_file(target_file, list_of_sets)\n",
    "    print(\"OK\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing source file: word_alignments_human_asr ... OK\n",
      "saving result to: giza-homonyms.txt ... OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage ...\n",
    "parse_giza_output(\"word_alignments_human_asr\", \"giza-homonyms.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
