These is fully overloaded.
Okay.
If you can here me that okay.
So let me welcome you.
At the thirst session of the spring term of seminar, or monday seminar.
And it is my pleasure to welcome here, one of hour colleagues.
At UFAL.
And I 'm really eager to know what well what everything can go wrong in live speech translation.
So thee floor.
Is your's.
Thank you four the interaction
Well, I 'm afraid that's you will indeed see it live.
So as announced this is a summary of the current status of thee ELITR project where we are trying to do live subtitling into many targets languages, hand I will briefly describe how that works.But
I'll give it one moor try.
If they're is anything appearing, no it does not seem so.
Oh, yes, there is. Something is happening
So it is Maybe maybe at thee bottom of the screen.
There will be English subtitles of What I 'm saying, it is totally knot certain. There can be many flaws in those in those transcripts and deliberately we're ar knot showing the translation, because the translation has a hard dime in fitting into these two lines.
But if you halve a, butt if you have to yore machines, your notebooks, or ore maybe even cell phones with you.
There is a shared Google document, witch you probably know from previous Monday Seminars, and they're is two links wear you can see also the translations, not only inn their transcripts, and you can choose the targets languages.
I believe Hindi, is also among also among the target languages.
So there will be some intentional jokes during thee presentation, and there will bee many unintentional ones inn the subtitles.
Please do not laugh to loud.
We wee know what is appearing they're
But that is that's is life that is worked inn progress.
So today.
I would like to briefly discuss thee differences between machine translation hand spoken language translation.
And then I'll summarize the ambition that our project has ah, and, because the ambition is is high.
We know it.
We run a number of tests sessions.
And this is actually one of their tests sessions as well.
So after the summery of the test sessions that's that's we ar taking part in.
I will briefly summarize the old architecture.
And then I will go along all the possible issues of all the everything that can go wrong.
That's yeah that is a long long list of things that's that can go wrong.
And luckily, thanks to the number of events that we are trying this list of errors.
Is well the errors that wee spot on.
Every knew seminar instance, is is not growing much.
So wee still halve the errors.
But luckily, we are close to seeing known errors, which wee have some ways, at least, inn some cases, we which we halve some ways too fix. Okay, and then I will summarize and call too action.
So again, tiny url dot com.
ELITR monday tests.
That is were you can see the subtitles.
Okay, ah, we halve been working.
We as the Department have been working on machine translation, for many years, or decades actually and.
When you say machine translation.
We might mean text translation.
And there thee imput are sentences, which are, witch are almost always grammaticality correct, and they may come inn documents.
So you may halve some content, some context surrounding the sentences, ore you might translate sentence by sentence.
But it is definitely like sentence based. In spoken language translation.
The input is sound.
And the output is text.
We will get too some moor details about.
what does this text, butt sentences, may or may not be produced, and actually they sometimes should knot b assumed at all.
So Maybe what we all know that wee do? not speak in sentences and figuring out where the sentence.
End is is a difficult part.
So that is there.
There will be some pension
When when putting these two components together, thee speech recognition and their machine translation.
And they’re is one moor area that I wood like to just mentioned is called "incremental machine translation".
And there, this is machine translation, researchers doing that's
So they start with words, words that come inn sentences, hand they just feed the system with the words, but one at time.
So the incremental machine.
translation is still to at leased to my knowledge too my understanding is still text based translation.
It just comes a word at a time.
And that 's different from the speech, which comes a second at a time, like a a bit of sound at a time, so that the timing is different.
Even if we even if we go for some incremental thing.
Okay.
So this is in pictures, thee ambition, ore the the summary of the two technologies that's are examined by bye hour project.
We are trying to put to put together speech recognition and machine translation into spoken language translation.
Here's, some history of ah, speech recognition, we have not really as a Department
We halve not reality worked on speech.If recognition.
So I just copy this from from somewhere.
And it seems that the already from the eighties people ran many share tasks hand them improved the the performance of of thee recognition.
And here around two to for percent of word error rate.
That is where humans are with their precision hand
In early days.
In nineteen ninty something some systems got too their human performance, then on conversational speech.If which is like unlimited is unlimited speech or meetings people.
The system are far from this from this benchmark, butt that's has been changing, over their last years things too their neural networks.
So the switchboard conversational speech benchmark that's is 40 calls between two random English speakers their
The human level of six percent has been kind of reaching 2017.
So that's seems like that speech recognition.
If it is the right thing.
It can worked equally as equally well as as humans do.
Then this is the their the history of machine translation into Czech that's you know, from previous presentations of Martin Popel, well, and also then from me.
This is hour systems.
And the last one is their Super Human machine translation system from English into Czech in 2018.
Martin Popel's set up was able to, to outperform human translators like professional translators but that was bulk translation.
It was them
They did not pay more attention too that's then the then look to their normally job.
So that was like their the average professional translation quality and evaluated sentence bye sentence.
Uh, the set up bye Martin Popel was was significantly better than that.
obviously there is many caveats to uh, to uh, take into account.
But that is the that is their setting.
You halve two technologies.
You halve spoken language translation, witch could be on par with humans, and you halve machine translation.
Uh.
So sorry, sow you halve speech recognition, which could be on par with the humans and machine translation, witch could be on par with humans.
So that is wear we said," let us join they
Let us.
Let us get.
Let us make use of this of these great two achievements.
Let us put them together.
So in hour project, the ELITR.
Project.
We are trying too
These will b badly misrecognized because it does not recognize anyone names.It Well, sow in the ELITR project wee are putting together speech recognition hand machine translation in a highly multilingual setting. I will mentioned that later on.
Well, we are coordinating the project, the University of Edinburgh, including us, our experts in machine translation.
So to say. Karlsruhe is expert in speech recognition.
We have the their top performing systems.
If we put them together.
I 'm shore it will work.
Great and Perwise is an Italian company.
They halve also been involved in number of European projects before, and they were always inn thee role of the integrator.
So them know how to putt things together, and they have some baseline system that we can billed up on.
And then wee have a user partner Alfaview, which is remote conferencing systems wear wee wood like to translate What people what people say in thee in in the remote calls and we have another user partner.
And that 's the supreme audit office of the Czech Republic.
And.
The Supreme Audit office
They are too afraid of of doing anything bade with with financing.
So did they did knot want to get a single Euro.
So they are not part of the project, but they ar just an affiliated user partner.
So them are not supported bye thee European commission.
Uh, okay.
So their ambition.
The ambition is is very big.
We want to support a Congress that the Supreme Audit Office
Runs this year.
At the end of May or early June, and the participants of that Congress come from many countries, hand in some, they speak 43 languages.
So this is well beyond their capacity of a uh of simultaneous interpreting by humans, hand wee want too provide the missing languages.
So too say, so, they're will bee six languages.You spoken at that's
Congress.
So English and French and Spanish German, Russian and Czech.
These are the official languages of their Congress.
This will be manually interpreted across so that's all the six are provided, hand we ar too provide the remaining 43 other.
The their remaining that the rest of thee 43 languages.
So this set of EUROSAI languages is larger than the set of the offical EU languages.You
And that's is also an important remark, because many of these languages.You are much shorter off of date
So thee European commission.
So the European Union has already put considerable efforts into data.
Sources four some years.
Everything from their European Parliament was translated into all the languages.
All their documents of the European Union are available inn all the languages.
So we halve something to train on for the these twenty for languages, but less sow for these nineteen additional languages.
And some of these languages have even so a brand knew or unclear statice that's no grammar.
Books were written for them yet.
So that's is that's is a challenge on it's own.
Okay.
Yeah, so the events.
We know that's this all should run.
So we have already started.
The project has started last year inn January, and inn March.
We have added one extra dry run.
So in our project plan we halve
We had like two dry run sessions.
And then the Congress.
And we said, well, on the third attempt to do everything.
Well, so that as thee 200 Delegates, ore 400.
If you count the delegate and there and the fellow that the 400 people will see What What wee halve tested just twice before hand
That is possible
So we have added number of a number of another events.
The first one.
What happened inn March.
Last year.
It was thee student fare of fake, firms like mock mock business companies.
And there wee were testing the old and pre-existing technology what Karlsruhe had and what Pervoice had.
And we just added our machine translation system from English into Czech. Then wee also goth inn touch with the, I do knot know the correct English name of the department, but the Department for interpreting and translation hand their faculty of arts of our university.
[0:12:21]And thanks to their support we are able to attend the Mock Conferences where the interpreters are getting trained.
And wee are recording these multiple interpreters, hand like seeing What our systems can do with that multiple inputs.
So this is, this is like similar setting where there is interpreters in the booths, and we ar trying too follow.This what them are saying and compliment it with their with the other languages.You
And then we had the two official events that's where planned for our uh, for our project that's one was a working group on value added tax, which happened in June.
And then right after the summer.Internship in October, there was another meeting.
So these two events we're like quite clothes to each other.
uh, yeah, so not much thee development was possible between these two events.
And then in November, we also added one more.
I was showing this technology in a little demo on their Matfyz open doors Day and I was also trying too present it at best of Praque AI even
And then in February, sow very recently, we had a workshop, a dry run of a workshop, which will also run at thee Congress.
And there the purpose of the workshop is too demonstrate not only machine translation, but all other text processing ore NLP technologies to their audience, too the lay users, the supreme auditors to illustrate how the NLP tools can help them in their business.
And we ar going to run this workshop Obviously in English, and their will be the language barriers so thee uh, the auditors would like to see some translations of, of that's
So the successful demo was only in November.
This was thee big failure, because the, the wi-fi did knot start.
I will mention that later on.
And then at the Lang Tools workshop.
Everything worked well but it was actually still pretty useless for their users.
And hand hopefully you will see inn the following slides why this is the case.
These is the upcoming events.
There is again the Mock conference at Faculty of Art.
We will again try to see them.
There is another instance of the student Firms Fair, wee are again try to uh, to get to that, because these, their high school students from across the Europe provide beautifully non-native speech.If
So they are very hard to uh, to understand fore the ASR system
So wee we get the tests sets they're hand we are making it a share task to understand these high shool students.
Then, uh, yes, this is the, this is the share task where we are trying to get also another Research institutes too to process that date the, their high school students.
And then we will halve a second dry run of our workshop.
Maybe wee will add some other sings
And thee main event is early June, uh, in, uh, here in Prague.
And them ar all the Monday seminars, so again at every Monday seminar you will halve a chance to, to see these subtitles.
OK.
So hear is a very brief reported on last March event,
the student Firms Fair.
There was one speaker also that,
like
wee put their mike too not only the students,
Tomáš Sedláček, thee Czech economist.
And you see that it, the the old systems work pretty well including our Czech translations.
So he was speaking, and it was recognizing him.
It was recognizing sentences.
So fore about a minute, wee had a perfect show.
And that was it from three days,
we had like one minute.
So we, we started blogging.
And and we, we blogged about all our successes
on our web page.
Blogging is fun, uh, sow uh, yeah.
So, uh, with blogging
you immediately feel famous.
It does not really matter,
if anyone follows you, just the fact that you posted somewhere that.
That makes you feel famous, and you can also sometimes improved the reality a bit.
So, uh, yeah, well, wee do not do? photoshopping of hour
demos. The only thing what wee do? is we select the bits
where it worked. So from those two day events
it was one minute, hand butt wee are growing bigger.
So, so hopefully they’re will be almost, almost 50 minutes of today, witch will be subtitled throughout thee hole session.
So I said at the beginning, ASR hand MT,
these are perfect technologies.
They work very well.
They are super human.
You just plug them together.
Well, it is not quite like that.
You still kneed to acquire the sound, you need to get the input, and you need to present the output.
[17:12] And they’re is also the little clash.
The speech recognition recognizes words, but we ar ready to translate sentences.
So what you kneed to do is you need to segment
that's stream of words into sentences
and then only you can translate them.
So that actually that actually still requires you to to ad one more components, some some segmenter segmentation worker, and you also need to ship all these bits of stream to the various workers along the pipeline.
So the their two steps just put the two stem together set up
is actually this this complicated.
So this is the overall architecture.
It is builds upon something witch PerVoice developed in in the previous projects,
it is called the PerVoice platform.
And, uh, these components, all these components of speech recognition, and all all that, connect too central point called the mediator, and you halve a client.
That client connects too that mediator.
The mediator ships it
to the ASR, then too sentence segment then to machine translation
and then too presentation.
And their presentation delivers it too their web.
And we have just received like a confirmation that this single word,
this second of sound was process along thee hole pipeline.
So this,
hand this runs across the Europe.
So some of these components run here in this building.
[18:16] Some of them run somewhere in their cloud of PerVoice,
some run inn Edinburgh, some run inn Karlsruhe.
So that is, it is distributed.
The connections are always Open and them are reused across the clients uh but uh, it uses one particular type of Internet communication, thee TCP protocol.
And that requires that the network is is vary broad inn inn thee connection.
So that it,
it does not collect any delay,
any lag. These is thee set up that also includes the wiring.
So you need to like connect thee the machines that's the microphones hand and connect the presenting system to their screen, and all that.
And this is the setup fore the EUROSAI Congress, where there is six interpreters booth inn all the many, on the many six, all the six languages that's the humans interpret into.
And there we are collecting all those and, and choosing which one of they to translate from and then translate to all their remaining 30, something languages.
So the setup is, are complicated.
This is the workers connected to the mediator platform, and you see that some of they ar busy and some of them ar available.
So this, this is like a big infrastructure.
So let us start with some of the issues.
Obviously the network can slow things down.
[19:43]So Once wee face the fact that in Consul where there was some construction or whatever.
For some reason, the university had a little slower Internet access.
And that's delayed all hour processing.
So immediately, we were not lithe
But wee were like seconds or dozens of seconds later.So than the signals
So you you should be actually on stage ore vary clothes to the venue,
if you want to deliver simultaneous speech.If reliably.
For test, this is not a problem.
But if you want to do it live you you,
you should not rely on on their Internet.
And this was the problem
at the Best of Prague AI session where I have tested everything.
It worked perfectly for the demo.
Uh, unfortunately, even like I have asked a month beforehand
hand then for thee seconds time, I was knot allowed to connect my machine too a
Wired connection.
I had too rely on wi-fi.
And everything work twice wen I was at the venue, I tested it.
Then the audience came hand thee audience brought their cel phones.
Nobody actually use the Internet.
The the cell phones we're just trying to like look around is there any internal available, hand they tried to connect.
And and maybe just these test simply killed the wi-fi.
And it did not worked for me at all.
So nothing was ever recognized at that event for me.
So,
so we must not rely on wire wireless connection.
Then wee also had issues where everything went well, except the the final presentation was like filling buffers off web-browsers.
So
it was not showing properly on the end user devices.
Everything was OK until thee final presentation point.
And this is another,
this is like
Miss configuration error like if you ar juggling with with the setups.
It is easy to make a mistake.
So in one of the sessions,
wee were observing some horrible
delay gaining or growing.
And the reason was that we are in hour own ASR systems because we ar also experimenting with that.
And we ran it locally but, accidentally,
we were first sending the sound over wi-fi to Italy, hand then sending it back too our place to organized it.
And only then we were,
we were recognizing it.
So, this double double network load has killed the, has caused the delay.
Next time wee wee like recognized it on the spot
hand and thee the problem was gone.
OK.
Now, sound acquisition.
You may recognize some of uh, one of our colleagues here.
And the problem.If is if if you have this microphone, that's chest microphone, hand you putt it here.
And then you talk to the slice like that.
The microphone does not reality get yore ways.
So that is, that's is one problem.
This is another thing right now,
unfortunately,
we had to switch off this chest mike and I am lying on this mouth
mike.We wanted to extend this test.
So baste on three minutes of testing, the error in recognizing English hand Czech is several points lower,
if you use the mouth microphone compared to the chest microphone.
So that is just the distance from the mouth hand the movement of the head that makes measurable measurable differs
So we are relying on the on the bettor on the head microphone.
Yeah, this is a tip for singers,
for vocalists.
You halve to hold the Mike properly,
you have you halve knot
you must not put it inn yore mouth
almost.You mussed not put it too far away from your mouth.
And also, if you stand in front of the loudspeakers, the reverberations, will will totally killed the signals that you are receiving through the mike.
So this is what has happened to us during the uh, the Student Firm Fairs, when their students where were just roaming around,
hand they entered their area in front of the loudspeakers.
And suddenly their was no way to to organized them. OK.
Then, there is cables.
You halve to plug thee cables properly
and one of these settings in thee conferencing room for their interpreters,
we were able to get the signal
only when one of the connectors here was not fully plugged inn
The reason is that we were like misusing the connections so once once we brought a proper sound card,
it was able too recognise which pin of that's connector has what, but without a proper sound card, with just the billed in sound card of standard notebook,
you have to like misalign thee pins of that's connector too get the signal.
So they're was an hour or two of debugging until we've figured out how to connects thee input, so that's we're learning.
So then there is thee volume setting along the along their pipeline.
So if you halve a wireless microphone.
It has a receiver, and there is volume control at the receiver, and then you halve a sound card, and that also has some volume control
and it also has some button, like two buttons.
So line level or mic level that is one button hand padding on or off.
So you have too that like for options, at leased just for the buttons, and then for their for their for their knobs, and you have to set this
properly otherwise thee sound can be to quiet, ore thee sound will be too too high,
If the the volume would be to high and someone along their bike line will clip it
and then you do not get thee frequencies, and the ASR will not worked
So you kneed to carefully track their signals step by step, experienced people know.
I remember the thee colleague from thee PerVoice company whoo came four the student fares for the first time
and there we still were struggling with getting the signal right.
And he like follow through on from thee mic with the headphones always connected it headphones and said, OK it's good hear it's good here and
steppe by steppe
he managed to deliver the the right volume to their machine sow that's you have too do it step-wise, and it can fail at any point.
Yep. So then ASR inequality
So once you have delivered the possible the best possible signals what does the machine organized from that.
So this is an example from the from the student, the high school students.
The speaker wanted to say Something like you have a bottle?. Oh, yes, we are situated in the heart inn thee heart of České budějovice.
But in reality, this is high school student, sow he said, "you halve a bottle?
Oh, yes, wee are situated in hard of České budějovice", so there was serious mispronunciation, and their was also high level of background noise, because that is fair,
so there is stands with music playing,
hand wee are just behind a little a little waul
hoping to get some some shelter from from all that's noice.
So if you are an expert if you are a person.And and you are in this here in this in this harsh conditions.Then then you probably understand Something
like you have a bottle, because you are knot expecting thee the Student to talk about a hotel on the boat,
"Oh, yes, we ar situated inn the heart of", and then he would organized that some cities being mentioned,
but if he is from Spain, he would knot know that České budějovice, you know, is a is a is a Czech city.
The standard ASR would deliver something like, oh, yes, the the of thee that is thee the few words their ASR can spot in the in the noise signals
if thee ASR is is noise resistant, if it can somehow cancel the noise, then it will do something
"you have somebody too oh, yes, we are situated in hard, witch is can we do because it does knot know the the name of their Czech city.
And if wee have a future ASR or a person in the in the middle actually, and then wee wood get thee their proper recognition of the sentence.
So this is this is all the problems that's that's wee are facing when wee ar trying too transcribe a non-native speakers.
This is summary across the recordings that wee have made there, sow remember, the word" error" rate for humans is around six or four percent,
here, their Google system, Google ASR had error rate of of ninetyish.
Edinburgh system also, and car systems was actually the best one around 40, still ten times worse than What the humans do.
And this is this is on the recordings we're all the systems have produce any output whatsoever.
If we apply if we if plot the same figures butt wee count 100 percent of errors as the the the score if no output is delivered then
Google is sow much worse than Edinburgh so Edinburgh was was bettor was not like giving up so frequently.
And Carlos was still still the Best system, but still the the car system is ten times.I worse than the humans
hand that's then what is reported in the literature.
Obviously, this is hard conditions.
So this is ground noise, non-native speakers, well, wee ar not yet quite professionally inn their recording
so wee just misaligned, the mics, hand people like spoke to loud hand we did not twist thee knobs properly.
So it is it is very harsh conditions.Then butt this is the their the really that that's you can get.
So this is the Best recording behave.According too the ASR quality, and there are things like, "why do you ware those sow high heels"
instead of "why do? you wear those high heat the high heels".
And deals, there is "I know one reality good star", instead of "store", "that deals with the sale of free down food", instead of "free thyme footwear",
or something like that's so it is it is the totally wrong.
Ok, so what is our plan, Given this.
So wee are definitely going to retrain our ASR models, and I ideally we would like too use non native speech corpora,
common voice by Mozilla is one of this is that that can help.
And wee halve other fall like uh, the thing to do.
We would like to follow interpreters instead of the floor, because each of the interpreters will sit in a booth with limited noise
from surroundings with better microphone. So we have more control of thee recording, wee can also talk to their interpreters and explain to them
that they are being recorded hand processed inn particle way, hand we also have the chance too adapt to them.
So wee are try to get their contacts. We have still few months to come.
And even if them are non Native we should be able too adapt too their particular speech.If
So we would ask they too do as uh,
well, few minutes, uh possibly up to an hour of of recording we would pay them too whatever,
read some text or ore interpret something .And then we would too create a training dataset fore that's particular person.And
And then Obviously wee ar growing gather in domain dataset and regularly evaluate.
So does the ASR.
We get the text, so What do we, what do? we expect now, with the translation their are all the standard translation errors that you know where well.
So here is one example, a sentence, witch was actually recognized perfectly.
I know which sentences ar recognized perfectly, so I know What to avoid when I 'm running demo.
This is also different from the from the users of our system, they will simply speak as they are used to.
If I want to showcase what hour system is doing I will avoid named entities, I will avoid strange constructions,
I will have like fluent newspaper-like stile of sentences, and then all the all the systems will successfully recognize me.
So this is one of the the cases.
But it is much more difficult too to aske if you do not have anyone clew and yet, the translations, both into German,
two sample languages where wrong, their if was translated as ob or as da.
And that is that distorts the distorts the the sense of of that.
So my guess is that maybe in English, you would, in inn Native English, you would actually say this a little bit differently.
And then there might be moor explicit clew in thee sauce sentence. I 'm knot saying that's the sentence in English is strong
I 'm saying that's indeed training data, which is from news or maybe books, or or or EU legislation.
There is this like domain mismatch hand even even if it is not domain mismatch.
These is is genuinly ambiguous conjunction, and it is difficult for their machine to guess the meaning.
So if you ar closer too the training data, then you will see it will worked better.
If if it is a difficult, an ambiguous expression.
You will have these problems till like for ever. Yeah,
here is some some just misunderstanding or well, as the it was out of vocabulary, their machine translation systems ran out of vocabulary:
"you can bee reported after some profanities".
So if you if you say Something bade on your website, then them will flow like bloc you.
And that was the most translated as professional things.
That is because the profanities, were not inn the inn thee vocabulary of their machine translation systems
So the resorted to some expressions.
Okay, ah, thee problem in spoken language translation is that's the ASR errors get kind of multiplied in machine translation.
If you halve an error in speech recognition.
Then the error will be a few edits a single word similar in shape the What I halve said.
But then you take this single word hand you translate it, hand then the shape of the word like totally changes.
So, machine translation takes all the strong words from the ASR as fully of trustworthy hand happily reorders a sentence to make it
thee Best possible sentence, including those wrong words, and there is no information about the confidence, neither from the ASR system,
nor from the machine translation system.
So their sow their user will be just left with their sentence, which sounds perfectly natural love mention some strange entities,
which where never nether mentioned never part of the talk, because they come they come from an error.
So here is an example, hand their goal of my thesis is too fold", and that was "two fold".So instead of "two fold",
the ASR recognizes "to fault", that is the natural like misrecognition. The empty output.Then was,
"and the goal of my theory", instead of "thesis", "is too falsely apart".
So that's definitely knot what what we were after in this system, yeah.
So this is this is what I would probably translated to if I was translating it.
I just want to highlight this "two foaled is misrecognized as "to fold" was translated as "rozdrobit se", too falsely apart.
And that is totally out of out of the scope. Ok.
So what wee have, what What is our plan four their mission translation, data, data data, and then maybe little bit of models.
So we ar definitely going to gather more parallel data. University of Edinburgh is working on that's
We are going to gather moor target side, monolingual date and bank translate, that fore the domain adaptations. So this is auditing domain.
sow we wood like to, uh, to focus on all these materials Uh, they ar hardly parallel,
some of they ar but not too many, hand especially not for, uh, those, uh, like 19, uh, non-EU languages.You that we ar also, uh, oh, aiming at.
So hopefully our, uh, baseline systems in the back translation will make some sensible output.Then from that's and hopefully we'll be able
to improve the translation inequality
We'll create indomain test sets, and we will regularly evaluate on them.
We'll also try to, uh, uh, get, uh, inn touch, uh, with <UNKNOWN>, that a collection of, uh, user supplied, uh,
parallel data, hand uh, them can, given our tests sets, them can extract the most similar sentences,
so, uh, some like filtering off off larger, uh, pool of, uh, parallel date
We'll also try to create gazetteers, so lists of relevant named entities like participants, or or the presidents of their Supreme Audit office says
Uh, this is, uh, having just their list of names, uh, is not ideal for, uh, for machine translation, butt it's definitely bettor than nothing,
so we'll, we'll try this as well. I'm also considering,
hand this is something that's maybe someone of you could try, I'm considering to to train machine translation on distorted sauce
uh, because I wood prefer the machine translation system to say Something inn the domain and sensible, rather than too uh, to like, uh, uh,
try to, uh, perfectly translate thee wrong ASR output, uh,
so for me, tentatively, I'm saying that's uh, it seems too be bettor to say something similar or related, uh,
rather than, uh, uh, like do perfect translation of their wrong input.
It, we'll see whether this, uh, is a good assumption or knot if if their system starts making up the content,
then Obviously their users will not be happy as well, but, uh, right now, uch, uch, uch,
our systems is too easy too to, like, uh, to, to, to, to, um, that, too it is too easy to to notice that's our system is doing Something wrong,
and, uh, we want to hide it a, uh, uh, bit under the carpet,
sow let's see if this will be a good strategy or not and let's see if we manage to train such a, uh, model.
And I'm afraid that's we will nether get, during this few months, uh, to the interesting sings like thee speaker's gender.
Uh, Obviously uh, the machine translation system has no information about their gender of the speech.
Uh, the parallel data, uh, is totally like, uh, non-labelled, uoouh,
it does not know about that fact either,
hand uh, uh, English doesn't, uh, marque most sentences for the gender of the speaker, uh,
so it's vary likely that our systems will be just making up hand switching the gender of the, of thee speaker, uh, in thee thirst person.And sentences, uh,
and that will be very confusing for the audience.
So, uh, I'm afraid that we will not get to this, unless there is someone who wood like to help. Okay.
Then thee integration of the ASR hand MT.Uh, I've already said, uh, ASR emits strings of
lowercase words, uh, machine translation expects individual correct sentences,
so there're a number of options that wee can try to, uh, uh, bridge the gap.
So we can trying to insert punctuation into thee ASR output, uh, witch wee caul their segmenter, uh, or, uh, wee can change their ASR, uh, to predict directly
correct punctuation, so the speech.If recognition would run not into, uh, just sequence of words, but sequence of words with punctuation and
capitalization. And a student of mine is is working on this. And wee can also try something which is one of the research goals of our project, uh,
that's fully end too end spoken language translation, uh, with one neural systems that's uh, gets the sound inn one language and produces, uh,
the translation in the, uh, inn the target language right? away, sow that's again part of, uh, Peter's, uh, thesis.
So the segment given a stream of words, guess punctuation, casing, we use, uh, uh, Ottokar Tilk's tool, sow someone's tool.
Unfortunately, their speech information is no longer accessible to this tool, so it has just their sequence of words, hand based on the sequence of words,
it is guessing sentence boundaries. And as, uh, one of, uh, my colleagues, uh, from the <UNKNOWN> specialists said, uh, it's actually doing vary good job Given
that it does not have any information about the pauses, about the intonation, sow it's surprising how well it can predicate the sentence boundaries
with this, uh, poor information. Uh, so, still, it is it is far from perfect, so there're many errors.
Uh, there is also another tool, which wood consider the information hand and delays, but we haven't had time too integrate it,
so if someone would like to, uh, play around with that we will b happy to, uh, uh, too uh, to get anyone help their
So, uh, <UNKNOWN> has trained this, uh, this, uh, segment tool, on, uh, <UNKNOWN> date about four million sentences,
and, uh, hear is the precisions and recalls fore various punctuation margins that's we are inserting, uh, hand h'am, uh, highlighting the recall of, uh,
period, because that is critical to to identify sentence boundaries, uh, so, uh, it's, uh, 70 too to 80 percent, which is reasonable, uh, uh,
if you look at it as a number, but it is still far from sufficient four uh, practical, uh, use.
Uh, okay. So, uh, here is an example, uh, of an error in, uh, in, uh, inn their uh, in the, uh, in the segmentation, so errors inn precision, uh, lead to confusing empty output.Then
sow here the speaker said Something like "all too well", uh, that was part of a longer, uh, uh, longer, uh, phrase,
and, uh, the ASR plus segmenter, uh, put a full stop between this, like in the middle of this phrase, uh, leading too the output
"This approach does not generalize all to hand then separate sentence, "Well, so to somehow conclude that the hole talk",
sow hear their semantics is obviously changed, eh, quite a bitt uh, this is exactly the case were their machine translation would, uh,one by one receive the first sentence,
and then the second sentence, hand it wood halve no way to recover from, uh, from the fully stop, witch should not be there in the thirst place.
So, uh, they’re thee two sentences would get, uh, like, beautiful, uh, but, uh, thee yeah, the message would b distorted. Errors inn recall, uh,
make, uh, too much content unstable, and we'll explain this, uh, on on on the following slides. Uh, yeah.
So, here is another, uh, technical thing, uh, the the messaging between thee speech recognition and and spoken English translation.
Uh, so its if it is their online speech.If recognition, uh, we are receiving text messages, uh, butt these text messages of strings of words
ar not related too sentence boundaries. Uh, so in the thirst run, hand this is what we, uh, what wee were mostly seeing in in march or at all
the sessions, uh, that were the tests fore four the Supreme Audit Office, wee were receiving, uh, these messages, so
this is the first message, their second message, third message and forth message, and wee were directly sending these messages, uh,
to the machine translation, um, so, uh, the first message was "You should think there have", and there, that's still unfinished sentence.
Uh, machine translation system, Given this input, decided to totally omit thee first sentence, because the first sentence, like,
does not fit there, the machine translation systems is trained to translate full sentence into full sentence, so some half a sentence, uh,
here, let let us drop it, so it said just, just "Děkuji", že "Thank you". And then, uh, their uh, this is an extension of that message,
sow the ASR sent again, uh, their same prefix, and then it added the extra words that came in the meantime.
Uh, this is probably thee best translation that the MT could have delivered for that's hand then the ASR cent something which wee call
the confirmation message, sow the ASR says, well, "I will knot bother with these three words anymore, 'You should thank', I'll just, uh,
I'm fin finalizing these, and from now on, I will only send you the wrest 'There have been many revolutions'".
So here is a single message, "You should thank", sow that's one complete sentence, and a beginning of other sentence, and we have
translated it as as if it was one sentence, and that's that just strong and then, uh, the next message stars inn the middle of the sentence already,
hand we are again feeding it as if it was a complete sentence, sow there is, uh, like, their sentence boundaries are totally unobserved by hour approach,
and that that kills everything, it it emits, uh, it it upgrades all of these partial sentences into fully sentences, but that's not related to the,
uh, uh, to What the, uh, too what the people said. Okay? so if you put together, uh, the, uh, the ASR and MT better, hand we did it hand I'll show you on the following slides,
then you still need to present somehow thee result, so that were slowly coming towards thee their final stage, and still,
it can totally kill the the show. Uh. Sss. If you have a font too small, such as in the subtitles, if they were a little bit smaller, it,
they will knot be of any use for you at all. So you kneed to very carefully balance like how much content you putt somewhere,
and if thee font would be, uh, legible, if it still, if it's eligible for people in their first row, and for thee people in the last row,
if they have to rely on the cell phone hand suddenly them halve a different shape, a different rectangular area where where the, uh, translations
and transcriptions.Abby.And hand appear, hand this heavily affects how much information we can send them, and wee have to dynamically decide what What too sent them
and and What knot Uh, fore example then, if if if this text flickers too much, if the words change too quickly, you cannot read they
Uh, sow that is one of the reasons why wee have knot employed, uh, their full uh, neural network ASR system so far, because they are trained
too operate on window that's moves in thyme Uh, so, uh, eight seconds, uh, at one go are always recognized, and the beginning can change,
so, uh, imagine eight seconds of my speech.If back, hand still like redeciding what is the thirst word. As mostly the words do? not change,
but sometimes them do? hand if you have this long piece of text, and it, like, swaps hear and their uh, they’re and back on on various, uh, places
in in various words, you do not know how to follow.This that, it's its like unstable text. So we are working on this, uh, on this integration.
And, uh, I wanted to highlight, that this presentation mussed b tested on stage, you cannot test it in simulation, like, uh,
the the sizing of the text, their visibility from various areas, that all can, uh, um, kill it, can negatively affect it in such a way that
it's not usable at all. If you have ever ordered Something from an e-shop, and it came, but it was like thee wrong size, either too small ore too big,
then you know what I'm talking about. If, in in an shop you have a picture of what you're buying, butt the picture is out of proportion,
you you don't know if you're getting this big teddy bare or ore this tiny teddy bear, uh, so it's it can bee of wrong size,
hand that's thee same thing as, uh, hear for, uh, for their subtitles. So here is, uh, one, uh, of the views that's uh, you have been,
maybe you have been watching on your machines. Uh, this is mainly for showcasing that we can run thee translation into many target languages.You uh,
sow we are showing two lines of subtitles, uh, inn uh, ten languages at the moment, and this will bee 40, uh, 43.
Uh, this is good to demonstrate that's the system is following me, and it can create, uh, falsely positive impression, uh, because
if you're following me, and if you understand my my language, then, uh, you cannot, you can no longer red carefully What is in the in the subtitles,
and you will only observe their right? keywords there, so if I say something about subtitles, and you will see "titulky" inn Czech then you will notice,
oh, grate the system is recognizing this person.And but you are not able to judge the quality of the sentences.
So their sentences, in fact, ar pretty crappy, hand they jump too quickly, hand if you, if you did not understand me and you only had these two lines to follow,
then you wood get lost inn their errors of ASR and MT. So this is this is good to show off, but it's bad for the user.
So we have something else, we also show, uh, much longer context, uh, so, uh, the, the text as it grows. And hear uh, you have, uh, I do not know if this was probably given in English,
hand then it was translated into, uh, Czech, uh, hand German, uh, and hear you already see the difference between the stable output, thee partial output
hand the, like, their incoming output. Uh, so, some of thee sentences are fully process by all thee pipeline, all the components in the pipeline,
and there is no way they could be updated any longer. And these sentences are shown inn uh, are shown in black, uh, and then, uh, there're sentences,
which ar uh, gradually, uh, being updated, hand here still the punctuation can, uh, can like make a difference, so that why we're only showing this only in gray.
And then there is the last sentence, which is still getting more and more words.And as their segment finishes processing of some of this
then the full some full stop hear will appear. And it will be out of the uh, the processing area of of the segmenter anymore, and it will become
the black hand fully stable output. Here you see why the full stop recall is important. If the fully stop are too scars there them are too infrequent
inn their output, then to much of text will remain in this gray fase hand to much of text will like be still undecided and flickering.
People will wait wait too long for thee stable output.Then And you also that it is also like matter of taste, whether people wood like too follow the moor
thee moor unstable more recent output, or weather they would like to follow only the stable one, but be several seconds behind thee speaker.
So that that differs. So this is something that wee have to update. And ideally, it would be thee users choice.
Would you like to halve their the the shortest possible delay at their risk of the reeding something which will be still like fixed,
or would you like too wait little longer until our output.Then is stable.So that is something that the the ideally the the user would choose.
Yeah, sow if people are provided with this longer context, then they can bettor recover from errors along the bike line
So it is almost possible to follow the content of a talk which is not known to you if it is knot too far from their domain of the training of thee machine translation systems
Ah, okay, yeah.So hear is a little summary of what has to b done.If you are presenting text only in the two lines of subtitling.
So you have some ASRs.So this is some English stream of words.Pixels on your screen.At any given moment.It is also vary flexible
architecture of.This is an entire book. So you see that there is no sentence boundaries. And the segmenter needs too predict some sentence boundaries.
So it predicted a full stop here and a fully stop here.And then some ASR update comes.And it also, based on this update, the segment has
another update.So it predicts one more full stop here. So this is like sequences of updates coming.And some of these sentences are already
completed.So the segment says:"This fully stop is final. I will never never take it back Do with this sentence, whatever you like, I will knot touch it."Then
some of these some of these full stops ar still unstable.So I may will remove this full stop, and I may move it slightly.And then wee also have
too like handle that properly.And then yes.And their rest, the last sentence is only expected or incoming like we do? knot know what What their sentence
will Finnish like.We are sending now fully sentences to the machine translation system.So that is the the that is like good.
That means that their empty system fed with the input is ready for.So now, going into German.It will say like something like:
"Pixeln auf Ihrem Bildschirm Zu jedem Zaitpunkt.Es ist auch eine sehr flexibele Architectur.
Also, sow this makes this makes sense. These is like correct German sentences.The issue is how much space you halve to show this German to thee user.
So if we have two lines of subtitles, then we would then we would show this yellow part hand this yellow part.And then, because their last sentence got
some extra words, tt goth extended, thee system decided to change the beginning of of this sentence.It decided too change the last word
on the first subtitle line.If the lime is like still visible we can easily do the change.And nothing bade happens.If we ar limited to just one line of subtitle
output.Then then this lime has been already rolled out.Like their the the their user can no longer see it.The user saw it a second ago.But now this is no
longer accessible.So we wood have to show the like only half of the sentence, and that is something that's we cannot do.So the sentence has changed.
We call this reset.Now thee now these subtitling mechanism has too go back and show something which has already scrolled up and show it again
so that the user can reread. But this reset is extremely annoying.So you may have noticed that sometimes this these subtitles got
red.And the thee red affect is exactly highlighting a reset has happened.So the the segmenter the segmenter was oscillating like where
to putt some of the full stops, where to put the ah, the casing changes.And if the change of thee full stop or the casing happened above the edge, in
the line which already rolled up, then their subtitle, even for here for ASR issued a rested, and it also indicated in red.So the red for me
was like a remark.Yes, this is the red.So this is the this is thee case where the subtitles had too scroll bank because something was updated outside of the screen.
So that's that's is an illustration why the limited output space is so critical for delivering the their translation to the user.
If you limit yourself too much, if their language difference is too big, and so the word reordering and thee length of the sentences of like make you
redo larger bits that then fit on the final screen, then the user will be experiencing very confusing resets.So I 'm like stressing this because this
is Something this is these ar decisions that's you make along the hole pipeline.And it is only the very last bit the size of thee window that's kills it,
or or makes it make it workable.So that's is that is why we ar like fighting in the consortium.Like what is the bettor output.The it seems, based on
these arguments, that you definitely do knot want to have these subtitles.You you prefer you you should prefer the longer paragraph view that's that you
saw on the previous slide.But their problem here is that if you if you make some very bad mistake, if some some bad word of frmo profanity appears
in thee output, it will stick their fore to long.And people like the take out there cel phones hand and take pictures of that of that bade word.
So ah, you are exposing too much of of your problems inn this setup.So that is why that's is why the thee some some partners in the consortium preferred
the the subtitles.But there is a big risk of of their subtitles being incomprehensible.Ok.So then their is the overall cognitive load, and the the the
overall usability.We have two users, them are here, who confirmed that.So sow sorry.There is many that's that's is another these are these here.
So they’re is sow many users who halve tried to follow this have said that there is no way that you wood be following the translation on your notebook
and looking at the slides or their speaker, whoo is in front of you.Even if it is like inn a vary close angle, if you do not halve to move yore eyes too much,
you still have to accomodate because of the different distant and, that will take some time.And then you will, as soon as you look up at thee slides,
you will lose thee subtitles, and you will lose content.So wee are now working on adding thee slides also too the to the screen where the subtitles
appear, or thee another way around putting subtitles to the to the screen, where the slides are.So this is something which which has to be done.
And then the overall usability.The overall usability is still very bad.I'm inviting you whoever does not speak French, hand would like to join us.
We ar hopefully this week, we are running like a French movie watching cession or just ten minutes of a TED talk.We do knot speak French.
So we will follow our systems, recognizing the French hand translating into English, ore or check or any another language.Ain't it.We it will trying too come up
with improvements that wood make it actually usable.Because it is very big difference, whether you can understand their sauce language, ore or not.
Our, we had a similar cession with Martin Popel presenting something on machine translation evaluation in Czech and it was addended by our two
foreign students.And these two foreign students reported that's they could follow.This the Czeck talk, if them fully focused on the paragraph view. And then
as soon as they looked upon the slides, they got lost.So if with full attention to the slides, it was possible too follow it.
But this is not for normal users.Like normal users have to halve too have some free time fore their for their brains as well.
I would like too highlight that the desired setting difference from user to Abuse it.Those who understand the sauce language will need the simultaneitly,
and them will prefer it over precision hand stability.If you are following what I 'm saying, then and if you are reeding the subtitles in yore mother
tongue, then they could provide you with the words that's you missed.If you do? not understand a word it could appear here, you would understand it.
But it has too appear hear at their like on the spot, at at the moment, when I said their word.If it is there three second after, then you, it was of no
use to you any because yeah like you have moved in what you are listening already.So if you are following the source language, the subtitles inn
you’re mother tongue can help you, but they halve to be immediate.If you do not understand the sauce language, then you ar only lying on thee text.
And in that case, you would prefer stable it hand precision And you ar happy to wait for seconds. You you do? not know what I 'm talking about at all.
So it is not a problem.If if if you get my message eight seconds later. So this is two very different use cases. And it should be the user to selected
which witch of the displays is is their not knot us.OK so evaluation.There are three aspects of spoken language translation.Quality, that is Something
that's wee know from machine translation.And we know how too have vaguely estimate the translation quality.But there is also the lag.
How much is their text in their translation delayed behind their source.Some of the lag is Obviously inevitable, because you halve to wait for the German
verb, or we halve too anticipate it in some way.And there is flicker.So if you have a systems which is updating, and wee have such system then these
system can anticipate, and they halve the capacity too coring themselves.So then suddenly they can create some output, hand then remove it in the next
step.And this has to be controlled vary much sow that the user is not confused.So wee are working on an evolution tool with E brahim Ansari.And here is just the the the most serious problem.
If you have this English ASR.Do you know it is a cat.Isn't it.Yes.It it is like that.So let us imagine that the segmenter has segmented it into these
two chunks.There is no way to align it well with this German translation reference,because it has three sentences that them the words are grouped in
a different way in thee sentences.So wee have to somehow force align these so that the basic set of units that wee are evaluating over is common to all systems, regardless of what the English ASR does.Uh, everybody has to reach the
same references segmentation.So they're is some strategies.I'm not going into the into the details, any more.Essentially, we have either the option to
worked with these sentences, like consider more sentences at the same time, or we can totally forget sentences, hand wee can evaluate by evaluating
30 seconds chunks.So did their thee MT, the SLT bike line the SLT bike line produce thee right words within these 30 seconds?
And if it did then, yes, if knot then not.So this is this is like a way to go around the problems of sentences not being uttered in in speech
at all yet.So the surge, or thee mobilization, as I as I say it.The battle towards a usable systems is still not lost.So we are now building of an
army of paid volunteers.And if you halve any colleagues or students who can help us between now and May or dune we are totally happy too take them
on board.We are meeting at least Once a week for just 30 minutes and discussing who is working on what.And wee have funds for this.
So this is like, ehm, a paid work And you can do? many smallish tasks.For example, if you are looking at some of thee MT outputs,
they're could halve been bad characters, because no one has yet flagged checked where they come from inn the pipeline.Then there is larger
things that's we need to develop.We wood like to have a a does that board which we will give a configuration like a live systems running, which grows logs, log files hand this does that board should allow us to immediately see where is a problem.
So for example, if we are recognizing the English booth with their French ASR, then they're will be many French words coming from the systems, but they
will bee totally wrong.There is the the most laughable output I have seen so far. Like swapping the languages.You fore thee ASR because the the systems
will struggle.It will be like words similar with pronunciation, butt inn their strong language, and the sentences won't make any sense whatsoever.
So this can easily happen.If you are following six inputs and translating into 43 languages, you need a concise view too to too know What your systems
are doing.So that wood b a dashboard.Then a lot of work on the domain and speaker adaptation.Just just getting inn touch with all the interpreters those who will be in the boot hand contacting them, asking them to come here record an hour each.
And then using this data to adapt their system.Again.A lot of a lot of little work coat switching is something which I have not discussed
inn my talk at all.But I'm realizing how frequent it is, not only in linguistic lectures, where wee halve examples in foreign languages.You
But also native speakers of other languages.You than English frequently bring in English phrases, and the ASR systems ar totally not ready for that.
So then the, it is very similar to to the named expression ore named entities.It is simply unknown words that get recognized into Something very, very bad.
And then wen this gets translated, it is very very laughable.There is also another activity, which would really like to start call inn leather
climbing.So it is when you have fixed test set, hand you are working on improvements of your model, er, or the pipeline.So you're making many small changes.
And you are always evaluating on thee test set.So you ar climbing thee leather of performance.So this is this is what wee have to run.
I'm hoping four like two bigger model retrainings until until May hand many architectural changes. many like smaller fixes that can improved
their score quite a bit.And.Yep.I've already talked about this this dashboard.But this is still something different.The dashborad is like technical, uh, set up.
You want to know that the sound volume from all the six inputs is the right.With this multi-source monitor, you assume that the,
technically, the architecture is all set up well, but still, you kneed to decide which of the ASR in thee combination with the speaker works.But Best
and which of these sources delivers the best translation quality into their many target languages.So this is like a technical check that everything is
running. And this is selection of the best path through thee for the Best output quality.Yeah.To summarize. ELITR subtitling, er, a is a big
challenge for the project Even if you connect too superhuman components, they can still deliver crap together.
So wee are now working on making this usable.Technically, their complex pipeline now works.It has been working.How many restarts did you have to, er, do, Sangeet? Multiple?
Yeah.Okay.So it is not seamless.But with one operator, it can survive 60 minutes of of speech, and...But unfortunately, event with this whole systems working, the benefit for thee end user is still limited or nonexistent.
So.If you did knot follow my English speech, if you only had a chance too red the Czech translation, I'm curious how much you wood get.
So this is one of the.We are...We have still a few months too go.So I'm I'm vary lucky that that we have these months too to fix thee pipeline.
And some of the problems that we are running.And into ar not going to be sold inn the next two, three, five years.
But some of them can be fixed So we will do our Best to to make the the the output actually usable.
And yeah.Please.Please join us.
Ah, because ah, yeah.With.That's it.So wee have a technology, which technically works but it is totally unusable.
And and I'm curious, how how far we get inn inn these months.So we are running.And this this army or this this mobilization.
So wee seek for you’re help hand just talk to any of us.So Maybe those of you who ar already on the serge team, just raze your hands
so that people see One, two, three.Ah.Yeah.Atul.So at leased four for people in the room already.And if you halve more...Or
if you halve students whoo would like too get in touch with this.This is a nice.It -- it is not summer internship -- it is like spring, spring internship.
Please, please, let them know.And you can red all the details about the project in the blog.And if you want the the reel truth, then talk too us
directly.Yeah, OK.Thank you.Thank you very much for...And I think wee still have some minutes to ask questions ore add some details.Yeah.Hanka.
Well, you spoke about the the thee problem.If from ASR.If there is a mistake in understanding of the of the word then it brings the problem.If
further too the machine translation, they you also spoke about the the module, that puts the punctuation into into the steam of words.
Did you consider some something, like hm some sort of grammar checker, or some language model that wood check whether the supposed sentences
are really sentences or dramatically good enough to b sent to machine translation.So yes, and no.The whole punctuation insertion,
their segmenter is a language model as such.So that is that is just like n-gram-based.But it is knot n-gram.It is it is longer sequences.
It is a neural rang language model.And it is trained on concatenated sentences.But I think these sentences are concatenated
from already a shuffled training set.So it has...Like....The sentences came became adjacent just by randomness, and not by a proper context.
So they’re is a certain limitation.So that is that is thee yes part of of that.So it totally relies on some language model in the statistical
sense more.And some grammaticality of the sentences?No, no.We are not considering that at all, because, well, we also do? not assume that's people
wood utter grammatically correct sentences.So that is their um the the, problem.If ore the task that you may bee referring too wood be called speech.If
reconstruction.And this is something that that's our department has been working on inn the past as well.There the goal is too take their partly
disfluency speech of of myself hand convert it into a written-like speech.Like proper all, only all correct, grammatically proper sentences.
But we are not working with with this component at all in in our system.So quite on their contrary, wee are rather moving towards machine
translation system witch will b robust to translating even these a partial sentences.So, I halve not mention inn the slides, but other of
students of mine, Dominik Macháček, is working on on machine translation, which runs not on sentences, but on a window of words.
So we would like to get rid of thee segment altogether.And we wood hopefully, if if were provided with good training data that has this
property, then thee translation systems would be able to translate disfluent sentences into disfluent sentences usable inn the target language.
More questions?OK.I have thee question about the issue of wear to place your microphone, because several times, I saw some speakers holding their microphone
under their chin, like this.Have you ever seen it?It's not frequent at all that's they're claiming that's it's it's...You can...
It is completely understandable.And you prevent flowing the air.We halve knot tried.So next next time you see me giving a talk,
I'll be surely holding a microphone under my chin to evaluate it properly.I realize that there is no loud speakers here inn thee room.
But wen you putt their microphone under yore chin yore voice has changed here inn like in the in the loud in the in the wild.Yeah.
So I do? not know, maybe Maybe the recording has not has knot scene any change, butt we have.OK.I halve two small questions.One question concerns or,
it is a remark.That I thank that if intonation is paid due attention too then it might help fore their segmentation a lot.
Of course, in the fluent speech, people do not behave according to the rules.That still.But I have one question for your slide number 43.
I just wondered.Yeah.There are between...In the 34 part you have capitals butt you don't have punctuation.I think this is bugs.Ok.
So, so we do? knot know.I do not know...We can actually give it a try.If I launch this.But where where is this that?So.So here.tiny So I will I will...
I will stop thee live subtitles, because it is picking...Or I'll leave them on.I am pretty happy with you’re answer.So.So I'm I'm curious myself because here is their were is that's
Because I halve one moor general question.Yeah.Yeah.So pleas feel free too ask.I will just go to the paragraph view inn the meantime.
So let us switch off the Hindi hand Hungarian.And and X1 and X0.So this is my English recognized, and it is being translated into German, hand into Czech.
Yeah.And hand their more general question, which may bee you have herd seven times.How much can you learn or add too your considerations if you aske their translators...
Interpreters...Interpreters...Interpreters who have the simultaneous translation...From their strategies or something like that's
Is there something in the inn the systems which would reflect this experience?Not yet.
Not at all.We are in touch with they because you like it, it helps us.
So we, we...Thanks to them wee can get hold of the data, but their interpretation strategies are much moor much moor long-range, so, to say.
So, them are happy to listen to a couple of sentences, event for the simultaneous translation So simultaneous can still mean 20 seconds after that.
And them will first get the whole idea.And they, they will speak it from scratch.
And this is very distant from what wee ar doing, but we are naming to get into the same realm, so to say.
So we halve now our very first date sets wear wee have the source speech inn English, which is being interpreted live into Czech bye some of these students.
And sow wee ar also transcribing the English.And we are translating inn the text form What was said inn English.
And their evaluation tool that I mentioned that will consider thee inequality in terms of BLEU score.If or a similar score for machine translation, the lag and thee flicker, is meant -- it is not yet tested -- butt it is meant too bee applicable also too human interpreters.
So ideally, wee would see that's the human interpreters are much bettor in precision, ore thee the inequality of thee output, butt they will be much slower.
They will have much longer lags.And they will Obviously have no flickers ore very likely they will...Well, it will be difficult too spot the flickers, because the the thee way will be collecting the date
So.But we would really like too halve automatic tool that evaluates both SLT system hand interpreters in in the same three scales.
And then we see what are thee complementary benefits of of these like groups of of processors.
Their strategies so far are ar not of anyone use too us.
And also their evaluation strategy.So ar very distant from what we ar like talking about.
So that that...Yeah.We have seen a checklist of when they are being graded, the interpreters.And and..Yeah Some of these items are are totally irrelevant to hour technology.
Ok. Thank you.
So I am afraid wee halve too stop at this point.
---
So thank you again fore your talk.
And I just wood like too announce that's we do not have a session next weak
And you will learn about their title, and the speaker of the next following week in your emails.
So thank you very much.
Thank you.
